#!/usr/bin/env python3
"""
æ°´å°è¡°å‡éªŒè¯æµ‹è¯•
æµ‹è¯•å¾ªç¯åµŒå…¥vsåˆ†æ®µåµŒå…¥å¯¹æ°´å°å¼ºåº¦çš„å½±å“
"""

import os
import sys
import yaml
import torch
import time
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(__file__))

# å¯¼å…¥æ°´å°ç›¸å…³æ¨¡å—
from src.text_watermark.credid_watermark import CredIDWatermark

# å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼Œé¿å…è”ç½‘
os.environ.setdefault('TRANSFORMERS_OFFLINE', '1')
os.environ.setdefault('HF_HUB_OFFLINE', '1')


def _candidate_cache_dirs() -> list:
    """è¿”å›å¯èƒ½çš„æœ¬åœ°ç¼“å­˜ç›®å½•åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ã€‚"""
    candidates = []
    # 1) é…ç½®/ç¯å¢ƒæŒ‡å®š
    if os.getenv('HF_HOME'):
        candidates.append(os.path.join(os.getenv('HF_HOME'), 'hub'))
    if os.getenv('HF_HUB_CACHE'):
        candidates.append(os.getenv('HF_HUB_CACHE'))
    # 2) æœ¬é¡¹ç›®å†… models ç›®å½•
    candidates.append(os.path.join(os.path.dirname(__file__), 'models'))
    # 3) é¡¹ç›®ä¸Šå±‚å¸¸è§ç¼“å­˜è·¯å¾„
    candidates.append('/fs-computility/wangxuhong/limeilin/.cache/huggingface/hub')
    # 4) ç”¨æˆ·ä¸»é¡µé»˜è®¤ç¼“å­˜
    candidates.append(os.path.expanduser('~/.cache/huggingface/hub'))
    # å»é‡å¹¶ä¿ç•™é¡ºåº
    seen = set()
    ordered = []
    for p in candidates:
        if p and p not in seen:
            seen.add(p)
            ordered.append(p)
    return ordered

def load_test_config():
    """åŠ è½½æµ‹è¯•é…ç½®"""
    with open('config/text_config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # æ°´å°è¡°å‡æµ‹è¯•é…ç½®
    config['num_beams'] = 1  # åŠ é€Ÿæµ‹è¯•
    config['lm_params']['message_len'] = 10  # 10-bitç¼–ç 
    config['wm_params']['encode_ratio'] = 8   # é™ä½ç¼–ç å¯†åº¦ï¼Œç¡®ä¿è¶³å¤Ÿtokens
    # è°ƒæ•´å‰ç¼€é•¿åº¦é¿å…è·³è¿‡ç¬¬ä¸€æ®µ  
    if 'lm_prefix_len' in config:
        config['lm_prefix_len'] = 5
    config['max_new_tokens'] = 3000  # ç¡®ä¿è¶³å¤Ÿé•¿çš„æ–‡æœ¬ç”Ÿæˆ
    config['confidence_threshold'] = 0.5
    
    return config


def load_model_and_tokenizer(config):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model_name = config['model_name']
    print(f"ğŸ—ï¸  åŠ è½½æ¨¡å‹: {model_name}")
    
    start_time = time.time()
    cache_dir = config.get('hf_cache_dir')
    if not cache_dir:
        # å°è¯•è‡ªåŠ¨å‘ç°å¯ç”¨ç¼“å­˜ç›®å½•
        for c in _candidate_cache_dirs():
            if os.path.isdir(c):
                cache_dir = c
                break
    # å…è®¸ local_files_onlyï¼Œå®Œå…¨ç¦»çº¿
    local_only = True

    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            local_files_only=local_only,
            trust_remote_code=True,
            use_fast=True
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            local_files_only=local_only,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
    except Exception as e:
        tried = [f"model_name='{model_name}'", f"cache_dir='{cache_dir}'"]
        raise RuntimeError(
            "ç¦»çº¿åŠ è½½æ¨¡å‹å¤±è´¥ã€‚è¯·ç¡®è®¤æ¨¡å‹å·²å­˜åœ¨äºæœ¬åœ°ç¼“å­˜ï¼Œæˆ–åœ¨configä¸­è®¾ç½® 'hf_cache_dir' æŒ‡å‘æœ¬åœ°æƒé‡ç›®å½•ã€‚"\
            f" å°è¯•å‚æ•°: {', '.join(tried)}\nåŸå§‹é”™è¯¯: {e}"
        )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    load_time = time.time() - start_time
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ ({load_time:.1f}s)")
    
    return model, tokenizer


def print_detailed_confidence(extract_result, original_segments):
    """æ‰“å°æ¯æ®µæ°´å°çš„è¯¦ç»†ç½®ä¿¡åº¦ä¿¡æ¯"""
    print(f"ğŸ“Š é€æ®µç½®ä¿¡åº¦åˆ†æ:")
    
    if 'detailed_confidence' not in extract_result:
        print("   âš ï¸  æœªæ‰¾åˆ°detailed_confidenceä¿¡æ¯")
        return
    
    detailed_conf = extract_result['detailed_confidence']
    if not detailed_conf or len(detailed_conf) == 0:
        print("   âš ï¸  è¯¦ç»†ç½®ä¿¡åº¦åˆ—è¡¨ä¸ºç©º")
        return
    
    # ç›´æ¥ä½¿ç”¨æå–çš„æ¶ˆæ¯è¿›è¡Œåˆ†æï¼Œä¸éœ€è¦è½¬æ¢
    extracted_message = extract_result.get('extracted_message', '')
    
    match_count = 0
    total_confidence = 0
    
    # åªæ˜¾ç¤ºåŸå§‹æ®µæ•°çš„ç½®ä¿¡åº¦ï¼Œé¿å…æ˜¾ç¤ºå†—ä½™çš„å¾ªç¯æ®µ
    display_count = min(len(detailed_conf), len(original_segments))
    
    for i in range(display_count):
        if i < len(detailed_conf) and len(detailed_conf[i]) >= 3:
            conf_data = detailed_conf[i]
            abs_conf = conf_data[0]      # ç»å¯¹ç½®ä¿¡åº¦ 
            rel_conf = conf_data[1]      # ç›¸å¯¹ç½®ä¿¡åº¦
            prob_score = conf_data[2]    # æ¦‚ç‡åˆ†æ•°
            
            # åŸå§‹æ®µä¿¡æ¯
            orig_seg = original_segments[i] if i < len(original_segments) else "?"
            
            total_confidence += prob_score
            
            print(f"   æ®µ{i+1} : ç½®ä¿¡åº¦={abs_conf}, ç›¸å¯¹={rel_conf}, æ¦‚ç‡={prob_score:.3f}")
    
    # æ€»ç»“ç»Ÿè®¡
    avg_confidence = total_confidence / display_count if display_count > 0 else 0
    print(f"   âœ… æ€»ç»“: {display_count}æ®µåˆ†æå®Œæˆ, å¹³å‡æ¦‚ç‡={avg_confidence:.3f}")
   


def test_cyclic_embedding(message_segments, prompt, config, model, tokenizer):
    """
    æµ‹è¯•å¾ªç¯åµŒå…¥æ–¹æ³•ï¼ˆç°æœ‰æ–¹æ³•ï¼‰
    
    Args:
        message_segments: æ¶ˆæ¯æ®µåˆ—è¡¨ 
        prompt: æç¤ºæ–‡æœ¬
        config: é…ç½®
        model, tokenizer: æ¨¡å‹å’Œåˆ†è¯å™¨
        
    Returns:
        æµ‹è¯•ç»“æœå­—å…¸
    """
    print("ğŸ”§ æ–¹æ³•1: å¾ªç¯åµŒå…¥ï¼ˆç°æœ‰æ–¹æ³•ï¼‰")
    print("-" * 40)
    
    # é‡æ–°ç»„åˆå®Œæ•´æ¶ˆæ¯
    full_message = "".join(message_segments)
    print(f"ğŸ¯ å®Œæ•´æ¶ˆæ¯: '{full_message}'")
    print(f"ğŸ“ åˆ†æ®µ: {message_segments}")
    
    # ä½¿ç”¨ç°æœ‰çš„embedæ–¹æ³•ï¼ˆå†…éƒ¨å¾ªç¯åµŒå…¥ï¼‰
    watermark = CredIDWatermark(config)
    
    
    start_time = time.time()
    embed_result = watermark.embed(
        model, tokenizer, prompt, message_segments,  # ç›´æ¥ä¼ é€’æ®µåˆ—è¡¨
        segmentation_mode="auto"  # è®©ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
    )
    embed_time = time.time() - start_time
    
    if embed_result['success']:
        watermarked_text = embed_result['watermarked_text']
        # è®¡ç®—tokenæ•°é‡ç”¨äºéªŒè¯é•¿åº¦å……è¶³æ€§
        try:
            # ä½¿ç”¨æ¨¡å‹è‡ªå¸¦çš„tokenizerè®¡ç®—ç²¾ç¡®tokenæ•°
            tokens = tokenizer.encode(watermarked_text)
            token_count = len(tokens)
        except:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç²—ç•¥ä¼°ç®— 1 token â‰ˆ 4 chars
            token_count = len(watermarked_text) // 4
            
        print(f"âœ… åµŒå…¥æˆåŠŸ ({embed_time:.1f}s)")
        print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(watermarked_text)} å­—ç¬¦")
        print(f"ğŸ”¢ Tokenä¼°ç®—: ~{token_count} tokens")
        print(f"ğŸ¯ ç†è®ºéœ€æ±‚: {len(message_segments)} æ®µ Ã— {config.get('wm_params', {}).get('encode_ratio', 10) * config.get('lm_params', {}).get('message_len', 10)} = ~{len(message_segments) * config.get('wm_params', {}).get('encode_ratio', 10) * config.get('lm_params', {}).get('message_len', 10)} tokens")
        print(f"ğŸ“Š é•¿åº¦å……è¶³æ€§: {'âœ… å……è¶³' if token_count >= len(message_segments) * 100 else 'âš ï¸ å¯èƒ½ä¸è¶³'}")


        
        # æå–æµ‹è¯•
        print(f"\nğŸ” å¼€å§‹æå–...")
        extract_start = time.time()
        
        extract_result = watermark.extract(
            watermarked_text, model, tokenizer,
            candidates_messages=message_segments  # ä¼ é€’æ®µåˆ—è¡¨ä½œä¸ºå€™é€‰
        )
        
        extract_time = time.time() - extract_start
        
        # æ˜¾ç¤ºæå–ç»“æœè¯¦æƒ…
        print(f"ğŸ” æå–ç»“æœè¯¦æƒ…:")
        print(f"   æå–æˆåŠŸ: {'âœ…' if extract_result['success'] else 'âŒ'}")
        print(f"   æå–ç¼–ç : '{extract_result['binary_message']}'")
        # ä½¿ç”¨åŸå§‹åˆ†æ®µè®¡ç®—å¯¹åº”ç¼–ç ï¼Œé¿å…ä¸è‡ªåŠ¨åˆ†æ®µç»“æœé•¿åº¦ä¸ä¸€è‡´å¯¼è‡´çš„è¶Šç•Œ
        original_binary = watermark._message_to_binary(message_segments)
        print(f"   åŸå§‹ç¼–ç : '{original_binary}'")
       
        print(f"   æ•´ä½“ç½®ä¿¡åº¦: {extract_result['confidence']:.3f}")
        print(f"   æå–æ—¶é—´: {extract_time:.1f}s")
       
        
        # æ˜¾ç¤ºé€æ®µè¯¦æƒ…
        print()
        print_detailed_confidence(extract_result, message_segments)
        
        return {
            'method': 'cyclic',
            'full_message': full_message,
            'segments': message_segments,
            'embed_success': True,
            'embed_time': embed_time,
            'text_length': len(watermarked_text),
            'watermarked_text': watermarked_text,
            'extract_result': extract_result,
            'extract_time': extract_time,    
        }
    else:
        print(f"âŒ åµŒå…¥å¤±è´¥: {embed_result.get('error', 'Unknown')}")
        print(f"ğŸ¯ åŸå§‹æ¶ˆæ¯æ®µ: {message_segments}")
        return {
            'method': 'cyclic',
            'embed_success': False,
            'error': embed_result.get('error', 'Unknown')
        }


def test_watermark_attenuation():
    
    print("=" * 60)
    
    # æ£€æŸ¥CUDA
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name()
        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ”¥ CUDA: {device_name}")
        print(f"   æ˜¾å­˜: {memory_gb:.1f} GB")
    else:
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
    
    try:
        # åŠ è½½é…ç½®å’Œæ¨¡å‹
        config = load_test_config()
        print(f"\nğŸ“‹ æµ‹è¯•é…ç½®:")
        print(f"   æ¨¡å‹: {config['model_name']}")
        print(f"   message_len: {config['lm_params']['message_len']} (10-bitç¼–ç )")
        print(f"   encode_ratio: {config['wm_params']['encode_ratio']} ")
        
        print(f"\nğŸ—ï¸  åŠ è½½æ¨¡å‹...")
        model, tokenizer = load_model_and_tokenizer(config)
        
        # æµ‹è¯•ç”¨ä¾‹ - æ¯ä¸ªéƒ½æœ‰5æ®µæ¶ˆæ¯ï¼Œä½¿ç”¨é€‚ä¸­é•¿åº¦çš„prompté¿å…å¯¹é½é—®é¢˜
        test_cases = [
            {
                'name': 'ç‰ˆæœ¬å·æ ¼å¼',
                'message_segments': ['v', '2024', '1', '5', 'beta'],
                'prompt': 'Please provide a detailed analysis of the software release including version specifications, feature updates, compatibility requirements, and user documentation'
            },
            {
                'name': 'æ–‡æœ¬åºåˆ—', 
                'message_segments': ['hello', 'world', 'test', 'case', 'one'],
                'prompt': 'This example demonstrates natural language processing techniques including tokenization methods, semantic analysis, machine learning architectures, and practical implementation guidelines'
            },
            {
                'name': 'å­—æ¯åºåˆ—',
                'message_segments': ['A', 'B', 'C', 'D', 'E'], 
                'prompt': 'The sequence analysis contains algorithmic approaches, data structure implementations, optimization techniques, and performance benchmarking procedures for efficient processing'
            },
            {
                'name': 'æ•°å­—æ–‡æœ¬æ··åˆ',
                'message_segments': ['123', 'test', '456', 'demo', '789'],
                'prompt': 'The mixed content analysis shows statistical methodologies, data mining techniques, pattern recognition algorithms, and machine learning approaches for predictive modeling'
            }
        ]
        
        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
        print(f"\n" + "=" * 60)
        print("å¼€å§‹å¤šç”¨ä¾‹å¯¹æ¯”æµ‹è¯•")
        print("=" * 60)
        
        all_results = []
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nğŸ¯ æµ‹è¯•æ¡ˆä¾‹ {i}: {test_case['name']}")
            print(f"   æ¶ˆæ¯æ®µ: {test_case['message_segments']}")
            print(f"   å®Œæ•´æ¶ˆæ¯: {''.join(test_case['message_segments'])}")
            print(f"   æç¤ºæ–‡æœ¬: '{test_case['prompt']}'")
            
            # æ‰§è¡Œæµ‹è¯•
            result = test_cyclic_embedding(
                test_case['message_segments'], 
                test_case['prompt'], 
                config, model, tokenizer
            )
            result['test_name'] = test_case['name']
            all_results.append(result)
            
            if i < len(test_cases):
                print(f"\n" + "-" * 40 + " ä¸‹ä¸€ä¸ªæµ‹è¯• " + "-" * 40)
        
        # æ±‡æ€»æ‰€æœ‰æµ‹è¯•ç»“æœ
        print(f"\n" + "=" * 60)
        print("ğŸ“ˆ æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 60)
        
        success_count = sum(1 for r in all_results if r.get('embed_success', False))
      
        
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("ğŸš€ æ°´å°è¡°å‡éªŒè¯æµ‹è¯•\n")
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    import sys
    if len(sys.argv) > 1 and sys.argv[1] in ['--test', '-t', '--attenuation']:
        print("å¼€å§‹æ°´å°è¡°å‡éªŒè¯æµ‹è¯•...")
        test_watermark_attenuation()
    else:
        try:
            do_test = input("æ˜¯å¦è¿›è¡Œæ°´å°è¡°å‡éªŒè¯æµ‹è¯•ï¼Ÿ(éœ€è¦åŠ è½½å¤§æ¨¡å‹ï¼Œçº¦5-10åˆ†é’Ÿ) [y/N]: ").lower().strip()
            if do_test == 'y':
                test_watermark_attenuation()
            else:
                print("è·³è¿‡æµ‹è¯•")
        except EOFError:
            print("éäº¤äº’æ¨¡å¼ï¼Œè·³è¿‡æµ‹è¯•")
            print("æç¤ºï¼šä½¿ç”¨ 'python test_complex_messages_real.py --test' è¿›è¡Œæµ‹è¯•")
    
    print("\nâœ… ç¨‹åºç»“æŸ") 