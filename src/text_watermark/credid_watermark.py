"""
CredIDæ–‡æœ¬æ°´å°ç®—æ³•å°è£…
åŸºäºåŸå§‹CredIDå®ç°çš„é«˜çº§å°è£…ï¼Œæä¾›ç®€å•æ˜“ç”¨çš„åµŒå…¥å’Œæå–æ¥å£
"""

import torch
import numpy as np
from typing import Dict, Any, List, Optional, Union, Tuple
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    LogitsProcessorList, 
    PreTrainedModel, 
    PreTrainedTokenizer
)
import logging

# å¯¼å…¥CredIDæ ¸å¿ƒç»„ä»¶
from .credid.watermarking.CredID.message_model_processor import WmProcessorMessageModel
from .credid.watermarking.CredID.random_message_model_processor import WmProcessorRandomMessageModel
from .credid.watermarking.CredID.message_models.lm_message_model import LMMessageModel
from .credid.watermarking.CredID.message_models.random_message_model import RandomMessageModel


class CredIDWatermark:
    """
    CredIDæ–‡æœ¬æ°´å°ç®—æ³•å°è£…
    
    è¿™æ˜¯ä¸€ä¸ªé«˜çº§å°è£…ç±»ï¼Œéšè—äº†CredIDç®—æ³•çš„å¤æ‚æ€§ï¼Œæä¾›ç®€å•æ˜“ç”¨çš„APIã€‚
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. LMæ¨¡å¼ï¼šä½¿ç”¨è¯­è¨€æ¨¡å‹ä½œä¸º"ä¸“å®¶é¡¾é—®"ï¼Œè´¨é‡æ›´é«˜ä½†é€Ÿåº¦ç¨æ…¢
    2. Randomæ¨¡å¼ï¼šä½¿ç”¨éšæœºåŒ–è§„åˆ™ï¼Œé€Ÿåº¦æ›´å¿«ä½†è´¨é‡ç¨ä½
    
    Example:
        config = {
            'model_name': 'huggyllama/llama-7b',
            'mode': 'lm',  # or 'random'
            'lm_params': {...},
            'wm_params': {...}
        }
        watermark = CredIDWatermark(config)
        result = watermark.embed(model, tokenizer, "Hello world", "secret")
        extracted = watermark.extract(result['watermarked_text'])
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–CredIDæ°´å°å¤„ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å‚æ•°ï¼š
                - model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
                - mode: 'lm' æˆ– 'random'ï¼Œå†³å®šä½¿ç”¨å“ªç§CredIDæ¨¡å¼
                - lm_params: LMæ¨¡å¼å‚æ•°
                - wm_params: æ°´å°å¤„ç†å‚æ•°
                - device: è®¾å¤‡è®¾ç½®ï¼Œé»˜è®¤auto
        """
        self.config = config
        self.mode = config.get('mode', 'lm')  # é»˜è®¤ä½¿ç”¨LMæ¨¡å¼
        
        # è®¾å¤‡è®¾ç½®
        device_config = config.get('device', 'auto')
        if device_config == 'auto':
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device_config)
        
        # æå–å„ç§å‚æ•°
        self.model_name = config.get('model_name', 'huggyllama/llama-7b')
        self.lm_params = config.get('lm_params', {})
        self.wm_params = config.get('wm_params', {})
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.message_model = None
        self.wm_processor = None
        self.original_message_length = None  # è®°å½•åŸå§‹æ¶ˆæ¯çš„æ®µæ•°ï¼Œç”¨äºå¾ªç¯æå–
        
        logging.info(f"CredIDWatermark initialized in {self.mode} mode on {self.device}")
    
    def _reset_message_state(self):
        """
        é‡ç½®æ¶ˆæ¯ç›¸å…³çš„çŠ¶æ€ï¼Œç”¨äºå¤„ç†æ–°æ¶ˆæ¯æ—¶æ¸…ç†ä¹‹å‰çš„çŠ¶æ€
        """
        self.original_message_length = None
    
    def _setup_processors(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):
        """
        è®¾ç½®CredIDå¤„ç†å™¨
        æ ¹æ®é…ç½®çš„æ¨¡å¼é€‰æ‹©ä½¿ç”¨LMæ¨¡å¼è¿˜æ˜¯Randomæ¨¡å¼
        
        Args:
            model: é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
            tokenizer: å¯¹åº”çš„åˆ†è¯å™¨
        """
        try:
            if self.mode == 'lm':
                # LMæ¨¡å¼ï¼šä½¿ç”¨è¯­è¨€æ¨¡å‹ä½œä¸ºä¸“å®¶é¡¾é—®
                self.message_model = LMMessageModel(
                    tokenizer=tokenizer,
                    lm_model=model,
                    lm_tokenizer=tokenizer,
                    delta=self.lm_params.get('delta', 1.5),
                    lm_prefix_len=self.lm_params.get('prefix_len', 10),
                    seed=self.lm_params.get('seed', 42),
                    lm_topk=self.lm_params.get('topk', -1),
                    message_code_len=self.lm_params.get('message_len', 10),
                    random_permutation_num=self.lm_params.get('permutation_num', 50),
                    hash_prefix_len=self.lm_params.get('hash_prefix_len', 1),
               
                    shifts=self.lm_params.get('shifts', [21, 24, 3, 8, 14, 2, 4, 28, 31, 3, 8, 14, 2, 4, 28])
                )
                
            elif self.mode == 'random':
                # Randomæ¨¡å¼ï¼šä½¿ç”¨éšæœºåŒ–è§„åˆ™
                self.message_model = RandomMessageModel(
                    tokenizer=tokenizer,
                    lm_tokenizer=tokenizer,
                    delta=self.lm_params.get('delta', 1.5),
                    seed=self.lm_params.get('seed', 42),
                    message_code_len=self.lm_params.get('message_len', 10),
                    hash_prefix_len=self.lm_params.get('hash_prefix_len', 1),
                    device=self.device,
                    # Randomæ¨¡å¼éœ€è¦çš„é¢å¤–å‚æ•°
                    shifts=self.lm_params.get('shifts', [21, 24, 3, 8, 14, 2, 4, 28, 31, 3, 8, 14, 2, 4, 28])
                )
            else:
                raise ValueError(f"Unsupported mode: {self.mode}. Use 'lm' or 'random'.")
                
            logging.info(f"Message model ({self.mode}) initialized successfully")
            
        except Exception as e:
            logging.error(f"Failed to setup message model: {e}")
            raise
    
    def _create_wm_processor(self, message: str) -> Union[WmProcessorMessageModel, WmProcessorRandomMessageModel]:
        """
        åˆ›å»ºæ°´å°å¤„ç†å™¨
        
        Args:
            message: è¦åµŒå…¥çš„æ°´å°ä¿¡æ¯
            
        Returns:
            é…ç½®å¥½çš„æ°´å°å¤„ç†å™¨
        """
        try:
            
            original_binary = self._message_to_binary(message, 'auto')
            
            # è®°å½•åŸå§‹æ¶ˆæ¯é•¿åº¦ï¼Œç”¨äºæå–æ—¶çš„å¾ªç¯é™åˆ¶
            self.original_message_length = len(original_binary)
            
            # å¯ç”¨å¾ªç¯åµŒå…¥ï¼šæ‰©å±•æ¶ˆæ¯æ®µä»¥æ”¯æŒé•¿æ–‡æœ¬ç”Ÿæˆ
            max_tokens = self.config.get('max_new_tokens', 1800)  # å¤§å¹…å¢åŠ é»˜è®¤å€¼
            encode_len = self.lm_params.get('message_len', 10) * self.wm_params.get('encode_ratio', 8)
            needed_segments = (max_tokens + encode_len - 1) // encode_len

            # å…³é”®ä¿æŠ¤ï¼šé™åˆ¶æ®µæ•°ä¸è¶…è¿‡shiftæ¨¡å¼å¯æ”¯æŒçš„æ•°é‡ï¼Œé¿å…ä¸‹æ¸¸ç´¢å¼•è¶Šç•Œ
            # æ³¨æ„ï¼šshiftsæ•°ç»„çš„é•¿åº¦å†³å®šäº†æœ€å¤§æ”¯æŒçš„æ®µæ•°
            # ä½†ç”±äºå†…éƒ¨å®ç°å¯èƒ½æœ‰ç´¢å¼•åç§»ï¼Œæˆ‘ä»¬ä¿å®ˆåœ°ä½¿ç”¨ len(shifts) - 1
            default_shifts = [21, 24, 3, 8, 14, 2, 4, 28, 31, 3, 8, 14, 2, 4, 28]
            shifts = self.lm_params.get('shifts', default_shifts)
            # ä¿å®ˆé™åˆ¶ï¼šä½¿ç”¨ len(shifts) - 1 ä»¥é¿å…è¾¹ç•Œé—®é¢˜
            max_supported_segments = self.wm_params.get('max_segments', len(shifts) - 1 if shifts else 10)
            if needed_segments > max_supported_segments:
                needed_segments = max_supported_segments
            
            # å¦‚æœéœ€è¦æ›´å¤šæ®µï¼Œå¾ªç¯é‡å¤æ¶ˆæ¯
            if needed_segments > len(original_binary):
                binary_message = []
                for i in range(needed_segments):
                    binary_message.append(original_binary[i % len(original_binary)])
                # print(f"å¾ªç¯åµŒå…¥: {len(original_binary)} â†’ {len(binary_message)} æ®µ (æ”¯æŒ{max_tokens}tokens)")
            else:
                binary_message = original_binary            
            if self.mode == 'lm':
                processor = WmProcessorMessageModel(
                    message=binary_message,
                    message_model=self.message_model,
                    tokenizer=self.message_model.tokenizer,
                    encode_ratio=self.wm_params.get('encode_ratio', 10),
                    seed=self.wm_params.get('seed', 42),
                    strategy=self.wm_params.get('strategy', 'vanilla'),
                    max_confidence_lbd=self.wm_params.get('max_confidence', 0.5),
                    top_k=self.wm_params.get('top_k', 1000)
                )
            else:  # random mode
                processor = WmProcessorRandomMessageModel(
                    message=binary_message,
                    message_model=self.message_model,
                    tokenizer=self.message_model.tokenizer,
                    encode_ratio=self.wm_params.get('encode_ratio', 10),
                    seed=self.wm_params.get('seed', 42),
                    strategy=self.wm_params.get('strategy', 'vanilla'),
                    max_confidence_lbd=self.wm_params.get('max_confidence', 0.5),
                    top_k=self.wm_params.get('top_k', 1000)
                )
            
            return processor
            
        except Exception as e:
            logging.error(f"Failed to create watermark processor: {e}")
            raise
    
    def embed(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, 
              prompt: str, message: Union[str, List[int], List[str]], 
              segmentation_mode: str = 'auto') -> Dict[str, Any]:
        """
        åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­åµŒå…¥æ°´å°
        
        è¿™æ˜¯ä¸»è¦çš„åµŒå…¥æ¥å£ã€‚å®ƒä¼šï¼š
        1. è®¾ç½®CredIDå¤„ç†å™¨
        2. å°†æ°´å°å¤„ç†å™¨é›†æˆåˆ°ç”Ÿæˆè¿‡ç¨‹ä¸­
        3. ç”Ÿæˆå¸¦æ°´å°çš„æ–‡æœ¬
        
        Args:
            model: HuggingFaceè¯­è¨€æ¨¡å‹
            tokenizer: å¯¹åº”çš„åˆ†è¯å™¨
            prompt: è¾“å…¥æç¤ºæ–‡æœ¬
            message: è¦åµŒå…¥çš„æ°´å°ä¿¡æ¯ï¼ˆæ”¯æŒå­—ç¬¦ä¸²ã€æ•´æ•°åˆ—è¡¨ã€å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
            segmentation_mode: åˆ†å‰²æ¨¡å¼ ('auto', 'smart', 'whole', 'spaces')
            
        Returns:
            åŒ…å«æ°´å°æ–‡æœ¬å’Œå…ƒæ•°æ®çš„å­—å…¸ï¼š
            {
                'watermarked_text': str,     # å¸¦æ°´å°çš„ç”Ÿæˆæ–‡æœ¬
                'original_message': str,     # åŸå§‹æ°´å°ä¿¡æ¯
                'binary_message': List[int], # äºŒè¿›åˆ¶æ°´å°
                'prompt': str,               # è¾“å…¥æç¤º
                'success': bool,             # æ˜¯å¦æˆåŠŸ
                'metadata': dict             # é¢å¤–å…ƒæ•°æ®
            }
        """
        try:
            # 0. é‡ç½®æ¶ˆæ¯çŠ¶æ€ï¼Œå‡†å¤‡å¤„ç†æ–°æ¶ˆæ¯
            self._reset_message_state()
            
            # 1. è®¾ç½®å¤„ç†å™¨ï¼ˆå¦‚æœè¿˜æ²¡è®¾ç½®ï¼‰
            if self.message_model is None:
                self._setup_processors(model, tokenizer)
            
            # 2. åˆ›å»ºæ°´å°å¤„ç†å™¨
            wm_processor = self._create_wm_processor(message)
            
            # 3. é…ç½®ç”Ÿæˆå‚æ•° - ç§»é™¤é•¿åº¦é™åˆ¶æ”¯æŒé•¿æ–‡æœ¬ç”Ÿæˆ
            generation_config = {
                'max_new_tokens': self.config.get('max_new_tokens', 1800),  # å¤§å¹…å¢åŠ é»˜è®¤é•¿åº¦
                'num_beams': self.config.get('num_beams', 1),  # å‡å°‘beam searchåŠ é€Ÿç”Ÿæˆ
                'do_sample': self.config.get('do_sample', True),
                'temperature': self.config.get('temperature', 0.7),
                'pad_token_id': tokenizer.eos_token_id,
                'eos_token_id': tokenizer.eos_token_id
            }
            
            # 4. åˆ›å»ºlogitså¤„ç†å™¨åˆ—è¡¨
            logits_processors = LogitsProcessorList([wm_processor])
            
            # 5. ç¼–ç è¾“å…¥
            input_ids = tokenizer.encode(prompt, return_tensors='pt', add_special_tokens=True)
            input_ids = input_ids.to(model.device)
            
            # 6. ç”Ÿæˆå¸¦æ°´å°æ–‡æœ¬
            with torch.no_grad():
                output_ids = model.generate(
                    input_ids,
                    logits_processor=logits_processors,
                    **generation_config
                )
            
            # 7. è§£ç è¾“å‡º
            # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»é™¤è¾“å…¥promptï¼‰
            new_tokens = output_ids[0][input_ids.shape[1]:]
            watermarked_text = tokenizer.decode(new_tokens, skip_special_tokens=True)
            
            # 8. å‡†å¤‡è¿”å›ç»“æœ
            binary_message = self._message_to_binary(message, segmentation_mode)
            
            return {
                'watermarked_text': watermarked_text,
                'original_message': message,
                'binary_message': binary_message,
                'prompt': prompt,
                'success': True,
                'metadata': {
                    'mode': self.mode,
                    'model_name': self.model_name,
                    'input_length': input_ids.shape[1],
                    'output_length': len(new_tokens),
                    'generation_config': generation_config
                }
            }
            
        except Exception as e:
            logging.error(f"Failed to embed watermark: {e}")
            return {
                'watermarked_text': None,
                'original_message': message,
                'prompt': prompt,
                'success': False,
                'error': str(e),
                'metadata': {'mode': self.mode}
            }
    
    def extract(self, watermarked_text: str, 
                model: Optional[PreTrainedModel] = None,
                tokenizer: Optional[PreTrainedTokenizer] = None,
                candidates_messages: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        ä»æ°´å°æ–‡æœ¬ä¸­æå–æ°´å°ä¿¡æ¯
        
        Args:
            watermarked_text: å¯èƒ½åŒ…å«æ°´å°çš„æ–‡æœ¬
            model: è¯­è¨€æ¨¡å‹ï¼ˆLMæ¨¡å¼éœ€è¦ï¼‰
            tokenizer: åˆ†è¯å™¨ï¼ˆLMæ¨¡å¼éœ€è¦ï¼‰
            candidates_messages: å€™é€‰æ¶ˆæ¯åˆ—è¡¨ï¼Œå¦‚æœæä¾›åˆ™åªæœç´¢è¿™äº›æ¶ˆæ¯
            
        Returns:
            åŒ…å«æå–ä¿¡æ¯å’Œç½®ä¿¡åº¦çš„å­—å…¸
        """
        try:
            # 1. æ£€æŸ¥æ¨¡å¼å’Œå‚æ•°
            if self.mode == 'lm' and (model is None or tokenizer is None):
                raise ValueError("LM mode requires both model and tokenizer for extraction")
            
            # 2. è®¾ç½®å¤„ç†å™¨ï¼ˆå¦‚æœè¿˜æ²¡è®¾ç½®ï¼‰
            if self.message_model is None:
                if self.mode == 'lm':
                    self._setup_processors(model, tokenizer)
                else:
                    # Randomæ¨¡å¼éœ€è¦ä¸€ä¸ªé»˜è®¤çš„tokenizer
                    from transformers import AutoTokenizer
                    default_tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                    if default_tokenizer.pad_token is None:
                        default_tokenizer.pad_token = default_tokenizer.eos_token
                    self._setup_processors(model, default_tokenizer)
            
            # 3. åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„æ°´å°å¤„ç†å™¨ç”¨äºè§£ç 
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¿æŠ¤åŸå§‹æ¶ˆæ¯é•¿åº¦ä¸è¢«ä¸´æ—¶å¤„ç†å™¨è¦†ç›–
            saved_original_length = self.original_message_length
            temp_message = "0"  # å ä½ç¬¦
            wm_processor = self._create_wm_processor(temp_message)
            # ç«‹å³æ¢å¤åŸå§‹çŠ¶æ€
            self.original_message_length = saved_original_length
            
            # 4. å‡†å¤‡å€™é€‰æ¶ˆæ¯ç¼–ç 
            if candidates_messages is not None:
                # ğŸ”§ ç®€åŒ–ï¼šæ”¶é›†æ‰€æœ‰å€™é€‰æ¶ˆæ¯çš„æ‰€æœ‰ç¼–ç 
                candidate_codes = []
                for msg in candidates_messages:
                    encoded = self._message_to_binary(msg, "smart")
                    if encoded:
                        candidate_codes.extend(encoded)  # æ·»åŠ æ‰€æœ‰æ®µçš„ç¼–ç 
                
                # å»é‡å¹¶æ’åº
                candidate_codes = sorted(list(set(candidate_codes)))
                
                # è½¬æ¢ä¸ºtensor
                candidate_messages_tensor = torch.tensor(candidate_codes, device=self.message_model.device)
            else:
                # å¦‚æœæ²¡æœ‰æä¾›å€™é€‰æ¶ˆæ¯ï¼Œåˆ™æœç´¢å…¨èŒƒå›´
                candidate_messages_tensor = None
                print(f"ğŸ” æœç´¢å…¨èŒƒå›´ 0-{2**self.lm_params.get('message_len', 10)-1} çš„æ‰€æœ‰ç¼–ç ")
            
            # 5. æ‰§è¡Œè§£ç 
            batch_size = self.config.get('decode_batch_size', 16)
            disable_tqdm = self.config.get('disable_tqdm', False)
            
            decoded_messages, (all_log_probs, detailed_confidences) = wm_processor.decode(
                watermarked_text,
                messages=candidate_messages_tensor,  # ä¼ å…¥å€™é€‰æ¶ˆæ¯
                batch_size=batch_size,
                disable_tqdm=disable_tqdm,
                non_analyze=False
            )
            
            # 6.  å¾ªç¯æå–é™åˆ¶ï¼šåªå–åŸå§‹æ¶ˆæ¯é•¿åº¦çš„æ®µæ•°
            if decoded_messages and len(decoded_messages) > 0:
                # å¦‚æœæœ‰è®°å½•çš„åŸå§‹æ¶ˆæ¯é•¿åº¦ï¼Œé™åˆ¶è§£ç ç»“æœçš„é•¿åº¦
                if self.original_message_length and self.original_message_length > 0:
                    if len(decoded_messages) > self.original_message_length:
                        decoded_messages = decoded_messages[:self.original_message_length]
                        print(f"ğŸ”§ å¾ªç¯æå–é™åˆ¶: æˆªå–å‰{self.original_message_length}æ®µ (åŸå§‹æ¶ˆæ¯é•¿åº¦)")
                    else:
                        print(f"ğŸ”§ æå–æ®µæ•°({len(decoded_messages)}) â‰¤ åŸå§‹é•¿åº¦({self.original_message_length})ï¼Œæ— éœ€æˆªå–")
                else:
                    print("âš ï¸  æœªæ‰¾åˆ°åŸå§‹æ¶ˆæ¯é•¿åº¦è®°å½•ï¼Œä½¿ç”¨å®Œæ•´è§£ç ç»“æœ")
            
            # 7. æ™ºèƒ½å¤„ç†å¤šæ®µè§£ç ç»“æœ
            if decoded_messages and len(decoded_messages) > 0:
                # å¦‚æœæœ‰å€™é€‰æ¶ˆæ¯ï¼Œè¿›è¡Œæ™ºèƒ½åŒ¹é…
                if candidates_messages is not None:
                    extracted_message, match_confidence = self._match_decoded_with_candidates(
                        decoded_messages, candidates_messages
                    )
                else:
                    # æ²¡æœ‰å€™é€‰æ¶ˆæ¯ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
                    if len(decoded_messages) == 1:
                        extracted_message = self._binary_to_message([decoded_messages[0]])
                    else:
                        extracted_message = self._binary_to_message(decoded_messages)
                    match_confidence = 0.0
                
                # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
                if detailed_confidences and len(detailed_confidences) > 0:
                    confidences = [conf[2] for conf in detailed_confidences if len(conf) > 2]
                    avg_confidence = np.mean(confidences) if confidences else 0.0
                    
                    # ç»“åˆåŒ¹é…ç½®ä¿¡åº¦
                    if match_confidence > 0:
                        final_confidence = (avg_confidence + match_confidence) / 2
                    else:
                        final_confidence = avg_confidence
                    
                    # åˆ¤æ–­æ˜¯å¦æˆåŠŸï¼ˆåŸºäºç½®ä¿¡åº¦é˜ˆå€¼ï¼‰
                    confidence_threshold = self.config.get('confidence_threshold', 0.6)
                    success = final_confidence > confidence_threshold
                else:
                    avg_confidence = 0.0
                    final_confidence = match_confidence  #  ä¿®å¤ï¼šä½¿ç”¨åŒ¹é…ç½®ä¿¡åº¦
                    success = final_confidence > self.config.get('confidence_threshold', 0.6)
                
                return {
                    'extracted_message': extracted_message,
                    'binary_message': decoded_messages,
                    'confidence': float(final_confidence),
                    'success': success,
                    'detailed_confidence': detailed_confidences,
                    'metadata': {
                        'mode': self.mode,
                        'text_length': len(watermarked_text),
                        'num_decoded_segments': len(decoded_messages),
                        'detection_method': 'CredID',
                        'confidence_threshold': confidence_threshold,
                        'search_space': len(candidate_codes) if candidates_messages else 2**self.lm_params.get('message_len', 10),
                        'candidates_provided': candidates_messages is not None
                    }
                }
            else:
                return {
                    'extracted_message': None,
                    'binary_message': [],
                    'confidence': 0.0,
                    'success': False,
                    'detailed_confidence': [],
                    'error': 'No watermark detected',
                    'metadata': {'mode': self.mode}
                }
                
        except Exception as e:
            logging.error(f"Failed to extract watermark: {e}")
            return {
                'extracted_message': None,
                'binary_message': [],
                'confidence': 0.0,
                'success': False,
                'error': str(e),
                'metadata': {'mode': self.mode}
            }
    
    def _message_to_binary(self, message: Union[str, List[int], List[str]], 
                          segmentation_mode: str = 'auto') -> List[int]:
        """
        å°†æ¶ˆæ¯è½¬æ¢ä¸ºCredIDå…¼å®¹çš„æ¶ˆæ¯æ ¼å¼
        
        æ”¯æŒå¤šç§æ¶ˆæ¯æ ¼å¼å’Œåˆ†å‰²æ¨¡å¼ï¼š
        1. å•ä¸ªå­—ç¬¦ä¸²ï¼šè½¬æ¢ä¸ºå•æ®µæ¶ˆæ¯
        2. å¥å­å­—ç¬¦ä¸²ï¼šåˆ†å‰²ä¸ºå¤šæ®µæ¶ˆæ¯
        3. æ··åˆå­—ç¬¦ä¸²ï¼šæ™ºèƒ½åˆ†å‰²å­—æ¯å’Œæ•°å­—éƒ¨åˆ†
        4. æ•´æ•°åˆ—è¡¨ï¼šç›´æ¥ä½¿ç”¨
        5. å­—ç¬¦ä¸²åˆ—è¡¨ï¼šæ¯ä¸ªå­—ç¬¦ä¸²è½¬æ¢ä¸ºä¸€æ®µ
        
        Args:
            message: è¾“å…¥æ¶ˆæ¯ï¼Œæ”¯æŒå¤šç§æ ¼å¼
            segmentation_mode: åˆ†å‰²æ¨¡å¼
                - 'auto': è‡ªåŠ¨åˆ¤æ–­ï¼ˆé»˜è®¤ï¼‰
                - 'smart': æ™ºèƒ½åˆ†å‰²æ··åˆå­—ç¬¦ä¸²  
                - 'whole': æ•´ä½“å¤„ç†
                - 'spaces': æŒ‰ç©ºæ ¼åˆ†å‰²
                - 'custom': è‡ªå®šä¹‰åˆ†å‰²ï¼ˆéœ€è¦é¢„å¤„ç†ï¼‰
            
        Returns:
            æ¶ˆæ¯æ•´æ•°åˆ—è¡¨
        """
        try:
            message_code_len = self.lm_params.get('message_len', 10)
            max_code = 2**message_code_len - 1
            
            # å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯
            if isinstance(message, str):
                # æ ¹æ®åˆ†å‰²æ¨¡å¼å¤„ç†å­—ç¬¦ä¸²
                if segmentation_mode == 'auto':
                    # è‡ªåŠ¨åˆ¤æ–­å¤„ç†æ–¹å¼
                    if ' ' in message and len(message.split()) > 1:
                        # åŒ…å«ç©ºæ ¼ï¼šæŒ‰å•è¯åˆ†å‰²
                        segments = message.split()
                    elif self._contains_mixed_content(message):
                        # æ··åˆå†…å®¹ï¼šæ™ºèƒ½åˆ†å‰²
                        segments = self._smart_segment_string(message)
                    else:
                        # ç®€å•å­—ç¬¦ä¸²ï¼šæ•´ä½“å¤„ç†
                        segments = [message]
                
                elif segmentation_mode == 'smart':
                    # å¼ºåˆ¶æ™ºèƒ½åˆ†å‰²
                    segments = self._smart_segment_string(message)
                
                elif segmentation_mode == 'spaces':
                    # æŒ‰ç©ºæ ¼åˆ†å‰²
                    segments = message.split() if ' ' in message else [message]
                
                elif segmentation_mode == 'whole':
                    # æ•´ä½“å¤„ç†
                    segments = [message]
                
                else:
                    # é»˜è®¤æƒ…å†µ
                    segments = [message]
                
                # è½¬æ¢æ¯ä¸ªç‰‡æ®µä¸ºæ•´æ•°
                messages = []
                for segment in segments[:10]:  # é™åˆ¶æœ€å¤š10æ®µ
                    segment_hash = hash(segment) % (2**16)
                    binary_str = format(segment_hash, '016b')
                    segment_code = binary_str[:message_code_len]
                    msg_int = int(segment_code, 2)
                    messages.append(msg_int)
                
                return messages
                    
            elif isinstance(message, list):
                if all(isinstance(item, int) for item in message):
                    # æ•´æ•°åˆ—è¡¨ï¼šç›´æ¥ä½¿ç”¨ï¼ˆç¡®ä¿åœ¨èŒƒå›´å†…ï¼‰
                    return [min(max_code, max(0, msg)) for msg in message]
                elif all(isinstance(item, str) for item in message):
                    # å­—ç¬¦ä¸²åˆ—è¡¨ï¼šæ¯ä¸ªè½¬æ¢ä¸ºä¸€æ®µ
                    messages = []
                    for item in message:
                        item_hash = hash(item) % (2**16)
                        binary_str = format(item_hash, '016b')
                        segment = binary_str[:message_code_len]
                        msg_int = int(segment, 2)
                        messages.append(msg_int)
                    return messages
                else:
                    raise ValueError("List must contain only integers or only strings")
            else:
                raise ValueError(f"Unsupported message type: {type(message)}")
                
        except Exception as e:
            logging.error(f"Failed to convert message to binary: {e}")
            # è¿”å›åŸºäºæ¶ˆæ¯å­—ç¬¦ä¸²ç®€å•å“ˆå¸Œçš„é»˜è®¤å€¼
            fallback_hash = hash(str(message)) % (2**message_code_len)
            return [fallback_hash]
    
    def _contains_mixed_content(self, text: str) -> bool:
        """
        æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦åŒ…å«æ··åˆå†…å®¹ï¼ˆå­—æ¯+æ•°å­—ï¼‰
        
        Args:
            text: è¾“å…¥å­—ç¬¦ä¸²
            
        Returns:
            æ˜¯å¦åŒ…å«æ··åˆå†…å®¹
        """
        import re
        
        # æ£€æŸ¥æ˜¯å¦åŒæ—¶åŒ…å«å­—æ¯å’Œæ•°å­—
        has_letters = bool(re.search(r'[a-zA-Z]', text))
        has_digits = bool(re.search(r'\d', text))
        
        # å¦‚æœå­—ç¬¦ä¸²è¾ƒé•¿ä¸”åŒ…å«æ··åˆå†…å®¹ï¼Œåˆ™è®¤ä¸ºéœ€è¦åˆ†å‰²
        return has_letters and has_digits and len(text) > 6
    
    def _smart_segment_string(self, text: str) -> List[str]:
        """
        æ™ºèƒ½åˆ†å‰²æ··åˆå­—ç¬¦ä¸²
        
        å°†åƒ"alibaba20250725"è¿™æ ·çš„å­—ç¬¦ä¸²åˆ†å‰²ä¸ºæœ‰æ„ä¹‰çš„ç‰‡æ®µ
        
        Args:
            text: è¾“å…¥å­—ç¬¦ä¸²
            
        Returns:
            åˆ†å‰²åçš„å­—ç¬¦ä¸²åˆ—è¡¨
        """
        import re
        
        # æ–¹æ¡ˆ1ï¼šæŒ‰å­—æ¯å’Œæ•°å­—çš„è¾¹ç•Œåˆ†å‰²
        segments = re.findall(r'[a-zA-Z]+|\d+', text)
        
        # æ–¹æ¡ˆ2ï¼šå¦‚æœæ•°å­—éƒ¨åˆ†å¾ˆé•¿ï¼Œè¿›ä¸€æ­¥ç»†åˆ†
        refined_segments = []
        for segment in segments:
            if segment.isdigit() and len(segment) >= 8:
                # é•¿æ•°å­—ï¼ˆå¦‚æ—¥æœŸæ—¶é—´æˆ³ï¼‰è¿›ä¸€æ­¥åˆ†å‰²
                # ä¾‹å¦‚ "20250725" -> ["2025", "0725"] æˆ– ["20250725"]
                if len(segment) == 8:
                    # å¯èƒ½æ˜¯æ—¥æœŸæ ¼å¼ YYYYMMDD
                    refined_segments.extend([segment[:4], segment[4:]])
                elif len(segment) >= 10:
                    # æ›´é•¿çš„æ•°å­—ï¼ŒæŒ‰4ä½åˆ†ç»„
                    for i in range(0, len(segment), 4):
                        refined_segments.append(segment[i:i+4])
                else:
                    refined_segments.append(segment)
            elif len(segment) > 8:
                # å¾ˆé•¿çš„å­—æ¯ä¸²ï¼Œåˆ†å‰²ä¸ºè¾ƒå°çš„ç‰‡æ®µ
                for i in range(0, len(segment), 6):
                    refined_segments.append(segment[i:i+6])
            else:
                refined_segments.append(segment)
        
        # è¿‡æ»¤ç©ºç‰‡æ®µå¹¶é™åˆ¶é•¿åº¦
        result = [seg for seg in refined_segments if seg and len(seg) > 0]
        return result[:10]  # æœ€å¤š10æ®µ
    
    def _binary_to_message(self, binary: List[int]) -> Union[str, List[str]]:
        """
        å°†CredIDè§£ç çš„æ•´æ•°æ¶ˆæ¯è½¬æ¢å›åŸå§‹æ ¼å¼
        
        Args:
            binary: CredIDè§£ç çš„æ•´æ•°åˆ—è¡¨
            
        Returns:
            è½¬æ¢åçš„æ¶ˆæ¯ï¼ˆå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰
        """
        try:
            if not binary:
                return "unknown"
            
            # å°è¯•åŒ¹é…å•æ®µæ¶ˆæ¯
            if len(binary) == 1:
                # ç®€å•çš„æµ‹è¯•æ¶ˆæ¯åˆ—è¡¨
                test_messages = ["hello", "test", "AI", "tech", "demo"]
                for test_msg in test_messages:
                    test_encoded = self._message_to_binary(test_msg, 'auto')
                    if test_encoded and test_encoded[0] == binary[0]:
                        return test_msg
                return str(binary[0])
            
            # å¤šæ®µæ¶ˆæ¯ç›´æ¥è¿”å›ç¼–ç è¡¨ç¤º
            return f"Multi-segment: {binary}"
                
        except Exception as e:
            logging.error(f"Failed to convert binary to message: {e}")
            return str(binary) if binary else "unknown"
    
    def _match_decoded_with_candidates(self, decoded_messages: List[int], 
                                     candidates_messages: List[str]) -> Tuple[str, float]:
        """
        å°†è§£ç ç»“æœä¸å€™é€‰æ¶ˆæ¯è¿›è¡Œæ™ºèƒ½åŒ¹é…
        
        Args:
            decoded_messages: CredIDè§£ç å‡ºçš„æ¶ˆæ¯æ®µåˆ—è¡¨
            candidates_messages: å€™é€‰æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            (åŒ¹é…çš„æ¶ˆæ¯, åŒ¹é…ç½®ä¿¡åº¦)
        """
        best_match = None
        best_confidence = 0.0

        for candidate in candidates_messages:
            candidate_encoded = self._message_to_binary(candidate, 'smart')
            confidence = self._calculate_sequence_match(decoded_messages, candidate_encoded)
            
            if confidence > best_confidence:
                best_match = candidate
                best_confidence = confidence
        
        if best_match and best_confidence >= 0.5:  # è‡³å°‘50%åŒ¹é…åº¦
            return best_match, best_confidence
        else:
            # æ²¡æœ‰å¥½çš„åŒ¹é…ï¼Œå°è¯•é‡æ„
            reconstructed = self._binary_to_message(decoded_messages)
            return reconstructed, 0.0
    
    
    
    def _calculate_sequence_match(self, decoded: List[int], candidate: List[int]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªåºåˆ—çš„åŒ¹é…åº¦
        
        Args:
            decoded: è§£ç å‡ºçš„åºåˆ—
            candidate: å€™é€‰åºåˆ—
            
        Returns:
            åŒ¹é…åº¦ (0.0 - 1.0)
        """
        if not decoded or not candidate:
            return 0.0
        
        # å®Œå…¨åŒ¹é…
        if decoded == candidate:
            return 1.0
        
        # é•¿åº¦åŒ¹é… + éƒ¨åˆ†åŒ¹é…
        if len(decoded) == len(candidate):
            matches = sum(1 for a, b in zip(decoded, candidate) if a == b)
            return matches / len(decoded)
        
        # å‰ç¼€åŒ¹é…ï¼ˆå¤„ç†æˆªæ–­æƒ…å†µï¼‰
        min_len = min(len(decoded), len(candidate))
        if min_len > 0:
            prefix_matches = sum(1 for a, b in zip(decoded[:min_len], candidate[:min_len]) if a == b)
            max_len = max(len(decoded), len(candidate))
            
            # ğŸ”§ ä¿®å¤ï¼šå‰ç¼€åŒ¹é…å¾—åˆ† = å®é™…åŒ¹é…æ•° / æœ€å¤§é•¿åº¦ * å‰ç¼€æƒé‡
            return (prefix_matches / max_len) * 0.8
        
        return 0.0
    
    def get_config(self) -> Dict[str, Any]:
        """è·å–å½“å‰é…ç½®"""
        return self.config.copy()
    
    def get_mode(self) -> str:
        """è·å–å½“å‰æ¨¡å¼"""
        return self.mode
    
 
    def reset(self):
        """é‡ç½®å¤„ç†å™¨ï¼Œæ¸…é™¤ç¼“å­˜"""
        self.message_model = None
        self.wm_processor = None
        logging.info("CredIDWatermark reset completed") 