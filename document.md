# ç®€åŒ–ç‰ˆå¤šæ¨¡æ€æ°´å°å·¥å…·è®¾è®¡

## ğŸ¯ é¡¹ç›®ç›®æ ‡

å¼€å‘ä¸€ä¸ªç®€å•æ˜“ç”¨çš„å¤šæ¨¡æ€æ°´å°å·¥å…·ï¼Œæ”¯æŒï¼š
- **æ–‡æœ¬æ°´å°**ï¼šåŸºäºCredIDç®—æ³•
- **å›¾åƒæ°´å°**ï¼šåŸºäºPRCç®—æ³•
- **è§†é¢‘æ°´å°**ï¼šåŸºäºVideo Sealç®—æ³•
- **ç»Ÿä¸€æ¥å£**ï¼šæä¾›ä¸€è‡´çš„åµŒå…¥å’Œæå–API

## ğŸ“ ç®€åŒ–ç›®å½•ç»“æ„

```
mmwt/                           # å¤šæ¨¡æ€æ°´å°å·¥å…·
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ text_config.yaml       # æ–‡æœ¬æ°´å°é…ç½®
â”‚   â””â”€â”€ image_config.yaml      # å›¾åƒæ°´å°é…ç½®
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ watermark_engine.py    # ç»Ÿä¸€æ°´å°å¼•æ“
â”‚   â”œâ”€â”€ text_watermark/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ credid_watermark.py # CredIDç®—æ³•å°è£…
â”‚   â”‚   â””â”€â”€ credid/             # CredIDç®—æ³•å®ç°ï¼ˆä»åŸé¡¹ç›®å¤åˆ¶ï¼‰
â”‚   â”œâ”€â”€ image_watermark/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ prc_watermark.py # PRCç®—æ³•å°è£…
â”‚   â”‚   â””â”€â”€ prc/         # PRCå®ç°ï¼ˆä»åŸé¡¹ç›®å¤åˆ¶ï¼‰
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config_loader.py    # é…ç½®åŠ è½½
â”‚       â””â”€â”€ model_manager.py    # æ¨¡å‹ç®¡ç†
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ text_demo.py           # æ–‡æœ¬æ°´å°æ¼”ç¤º
â”‚   â”œâ”€â”€ image_demo.py          # å›¾åƒæ°´å°æ¼”ç¤º
â”‚   â””â”€â”€ unified_demo.py        # ç»Ÿä¸€æ¥å£æ¼”ç¤º
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_text_watermark.py
â”‚   â””â”€â”€ test_image_watermark.py
â””â”€â”€ models/                    # é¢„è®­ç»ƒæ¨¡å‹å­˜å‚¨
```

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„è®¾è®¡

### ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

æœ¬å·¥å…·é‡‡ç”¨**åˆ†å±‚æ¨¡å—åŒ–æ¶æ„**ï¼Œä»ä¸Šåˆ°ä¸‹åˆ†ä¸ºï¼š
1. **ç”¨æˆ·æ¥å£å±‚**ï¼šæä¾›ç»Ÿä¸€çš„APIæ¥å£å’Œä½¿ç”¨ç¤ºä¾‹
2. **æ ¸å¿ƒå¼•æ“å±‚**ï¼šWatermarkEngineç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ°´å°æ“ä½œ
3. **ç®—æ³•å®ç°å±‚**ï¼šå…·ä½“çš„æ°´å°ç®—æ³•å°è£…å’Œå®ç°
4. **é…ç½®å’Œå·¥å…·å±‚**ï¼šé…ç½®ç®¡ç†ã€æ¨¡å‹ç®¡ç†ç­‰æ”¯æŒç»„ä»¶

### 1. ç»Ÿä¸€æ°´å°å¼•æ“ (WatermarkEngine)

**è®¾è®¡ç†å¿µ**ï¼š
- **å•ä¸€å…¥å£**ï¼šç”¨æˆ·åªéœ€è¦ä¸WatermarkEngineäº¤äº’ï¼Œæ— éœ€å…³å¿ƒåº•å±‚å®ç°
- **æ‡’åŠ è½½**ï¼šåªæœ‰åœ¨å®é™…ä½¿ç”¨æ—¶æ‰åŠ è½½å¯¹åº”çš„ç®—æ³•æ¨¡å—ï¼ŒèŠ‚çœå†…å­˜
- **é…ç½®é©±åŠ¨**ï¼šé€šè¿‡é…ç½®æ–‡ä»¶ç®¡ç†ä¸åŒç®—æ³•çš„å‚æ•°

**æ ¸å¿ƒå®ç°**ï¼š

```python
# src/watermark_engine.py
import os
import yaml
from typing import Optional, Dict, Any

class WatermarkEngine:
    """
    å¤šæ¨¡æ€æ°´å°ç»Ÿä¸€å¼•æ“
    
    åŠŸèƒ½èŒè´£ï¼š
    1. æä¾›ç»Ÿä¸€çš„æ–‡æœ¬å’Œå›¾åƒæ°´å°æ¥å£
    2. ç®¡ç†ç®—æ³•æ¨¡å—çš„æ‡’åŠ è½½
    3. å¤„ç†é…ç½®æ–‡ä»¶çš„åŠ è½½å’ŒéªŒè¯
    4. åè°ƒä¸åŒæ¨¡æ€é—´çš„æ“ä½œ
    """
    
    def __init__(self, base_dir: str = "."):
        """
        åˆå§‹åŒ–æ°´å°å¼•æ“
        
        Args:
            base_dir: é¡¹ç›®æ ¹ç›®å½•ï¼Œç”¨äºå®šä½é…ç½®æ–‡ä»¶
        """
        self.base_dir = base_dir
        self.text_watermark = None      # æ–‡æœ¬æ°´å°æ¨¡å—å®ä¾‹
        self.image_watermark = None     # å›¾åƒæ°´å°æ¨¡å—å®ä¾‹
        self._config_cache = {}         # é…ç½®æ–‡ä»¶ç¼“å­˜
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """
        åŠ è½½å¹¶ç¼“å­˜é…ç½®æ–‡ä»¶
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            è§£æåçš„é…ç½®å­—å…¸
        """
        if config_path not in self._config_cache:
            full_path = os.path.join(self.base_dir, config_path)
            with open(full_path, 'r', encoding='utf-8') as f:
                self._config_cache[config_path] = yaml.safe_load(f)
        return self._config_cache[config_path]
    
    def setup_text_watermark(self, config_path: str = "config/text_config.yaml"):
        """
        åˆå§‹åŒ–æ–‡æœ¬æ°´å°æ¨¡å—
        
        Args:
            config_path: æ–‡æœ¬æ°´å°é…ç½®æ–‡ä»¶è·¯å¾„
        """
        from .text_watermark.credid_watermark import CredIDWatermark
        config = self._load_config(config_path)
        self.text_watermark = CredIDWatermark(config)
    
    def setup_image_watermark(self, config_path: str = "config/image_config.yaml"):
        """
        åˆå§‹åŒ–å›¾åƒæ°´å°æ¨¡å—
        
        Args:
            config_path: å›¾åƒæ°´å°é…ç½®æ–‡ä»¶è·¯å¾„
        """
        from .image_watermark.stable_signature import StableSignatureWatermark
        config = self._load_config(config_path)
        self.image_watermark = StableSignatureWatermark(config)
    
    # === æ–‡æœ¬æ°´å°æ¥å£ ===
    def embed_text(self, model, tokenizer, prompt: str, message: str) -> Dict[str, Any]:
        """
        åµŒå…¥æ–‡æœ¬æ°´å°
        
        Args:
            model: é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (HuggingFace model)
            tokenizer: å¯¹åº”çš„åˆ†è¯å™¨
            prompt: è¾“å…¥æç¤ºæ–‡æœ¬
            message: è¦åµŒå…¥çš„æ°´å°ä¿¡æ¯
            
        Returns:
            åŒ…å«æ°´å°æ–‡æœ¬å’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        if not self.text_watermark:
            self.setup_text_watermark()
        return self.text_watermark.embed(model, tokenizer, prompt, message)
    
    def extract_text(self, watermarked_text: str) -> Dict[str, Any]:
        """
        æå–æ–‡æœ¬æ°´å°
        
        Args:
            watermarked_text: å¸¦æœ‰æ°´å°çš„æ–‡æœ¬
            
        Returns:
            åŒ…å«æå–ä¿¡æ¯å’Œç½®ä¿¡åº¦çš„å­—å…¸
        """
        if not self.text_watermark:
            self.setup_text_watermark()
        return self.text_watermark.extract(watermarked_text)
    
    # === å›¾åƒæ°´å°æ¥å£ ===
    def embed_image(self, model, prompt: str, message: str) -> Dict[str, Any]:
        """
        åµŒå…¥å›¾åƒæ°´å°
        
        Args:
            model: æ‰©æ•£æ¨¡å‹ (å¦‚ Stable Diffusion)
            prompt: å›¾åƒç”Ÿæˆæç¤ºè¯
            message: è¦åµŒå…¥çš„æ°´å°ä¿¡æ¯
            
        Returns:
            åŒ…å«æ°´å°å›¾åƒå’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        if not self.image_watermark:
            self.setup_image_watermark()
        return self.image_watermark.embed(model, prompt, message)
    
    def extract_image(self, watermarked_image) -> Dict[str, Any]:
        """
        æå–å›¾åƒæ°´å°
        
        Args:
            watermarked_image: å¸¦æœ‰æ°´å°çš„å›¾åƒ (PIL Image æˆ–è·¯å¾„)
            
        Returns:
            åŒ…å«æå–ä¿¡æ¯å’Œç½®ä¿¡åº¦çš„å­—å…¸
        """
        if not self.image_watermark:
            self.setup_image_watermark()
        return self.image_watermark.extract(watermarked_image)
    
    # === å·¥å…·æ–¹æ³• ===
    def get_config(self, config_type: str) -> Dict[str, Any]:
        """è·å–æŒ‡å®šç±»å‹çš„é…ç½®"""
        config_map = {
            'text': 'config/text_config.yaml',
            'image': 'config/image_config.yaml'
        }
        return self._load_config(config_map[config_type])
    
    def reset(self):
        """é‡ç½®å¼•æ“ï¼Œæ¸…ç©ºç¼“å­˜"""
        self.text_watermark = None
        self.image_watermark = None
        self._config_cache.clear()
```

### 2. æ–‡æœ¬æ°´å°æ¨¡å— (CredID Algorithm) âœ… **å·²å®ç°**

**CredIDç®—æ³•åŸç†**ï¼š
- **å¤šä½æ°´å°**ï¼šæ”¯æŒåµŒå…¥å¤šæ®µä¿¡æ¯ï¼ˆå¦‚ç”¨æˆ·IDã€æ—¶é—´æˆ³ã€ç‰ˆæœ¬å·ç­‰ï¼‰
- **logitså¤„ç†**ï¼šåœ¨è¯­è¨€æ¨¡å‹çš„logitsè¾“å‡ºä¸Šè¿›è¡Œä¿®æ”¹ï¼Œå½±å“tokené€‰æ‹©æ¦‚ç‡
- **åŒæ¨¡å¼æ”¯æŒ**ï¼šLMæ¨¡å¼ï¼ˆé«˜è´¨é‡ï¼‰å’ŒRandomæ¨¡å¼ï¼ˆé«˜æ•ˆç‡ï¼‰
- **å€™é€‰ä¼˜åŒ–**ï¼šæ”¯æŒå€™é€‰æ¶ˆæ¯åˆ—è¡¨çš„é™åˆ¶æœç´¢ï¼Œæå‡æ£€æµ‹æ•ˆç‡
- **æ™ºèƒ½åˆ†å‰²**ï¼šè‡ªåŠ¨å¤„ç†å¤æ‚æ¶ˆæ¯æ ¼å¼ï¼ˆå¦‚"log20250725143000"ï¼‰

**å®é™…å®ç°çš„æ ¸å¿ƒæ¶æ„**ï¼š

```python
# src/text_watermark/credid_watermark.py
import torch
import logging
from typing import Dict, Any, List, Optional, Union, Tuple
from transformers import PreTrainedModel, PreTrainedTokenizer, LogitsProcessorList

class CredIDWatermark:
    """
    CredIDæ–‡æœ¬æ°´å°ç®—æ³•ç»Ÿä¸€å°è£…
    
    âœ¨ æ ¸å¿ƒåŠŸèƒ½ç‰¹ç‚¹:
    1. æ”¯æŒå¤šç§æ¶ˆæ¯æ ¼å¼ (å­—ç¬¦ä¸²ã€æ•´æ•°åˆ—è¡¨ã€å­—ç¬¦ä¸²åˆ—è¡¨)
    2. åŒæ¨¡å¼è¿è¡Œ: LMæ¨¡å¼(é«˜è´¨é‡) / Randomæ¨¡å¼(é«˜æ•ˆç‡)
    3. æ™ºèƒ½å¤šæ®µæ¶ˆæ¯å¤„ç†å’Œè‡ªåŠ¨åˆ†å‰²
    4. å€™é€‰æ¶ˆæ¯ä¼˜åŒ–æœç´¢æœºåˆ¶
    5. å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œç½®ä¿¡åº¦è¯„ä¼°
    6. ç®€åŒ–çš„ä»£ç ç»“æ„ï¼Œå»é™¤å¤æ‚çš„æŒ‰ä½ç½®åˆ†ç»„é€»è¾‘
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–CredIDæ°´å°å¤„ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼Œå¿…é¡»åŒ…å«:
                - mode: 'lm' æˆ– 'random' (é»˜è®¤'lm')
                - model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
                - lm_params: LMæ¨¡å¼å‚æ•°å­—å…¸
                - wm_params: æ°´å°å¤„ç†å‚æ•°å­—å…¸
                - å…¶ä»–ç”Ÿæˆå‚æ•° (max_new_tokens, num_beamsç­‰)
        """
        self.config = config
        self.mode = config.get('mode', 'lm')  # é»˜è®¤LMæ¨¡å¼
        self.model_name = config.get('model_name', 'huggyllama/llama-7b')
        
        # ç®—æ³•æ ¸å¿ƒå‚æ•°
        self.lm_params = config.get('lm_params', {})
        self.wm_params = config.get('wm_params', {})
        
        # å»¶è¿Ÿåˆå§‹åŒ–çš„ç»„ä»¶
        self.message_model = None
        self.tokenizer_ref = None
        
        logging.info(f"CredIDåˆå§‹åŒ–: æ¨¡å¼={self.mode}, æ¨¡å‹={self.model_name}")
```

**ğŸ”¹ æ ¸å¿ƒæ¥å£ 1: embed() - æ°´å°åµŒå…¥**

```python
    def embed(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, 
              prompt: str, message: Union[str, List[int], List[str]], 
              segmentation_mode: str = 'auto') -> Dict[str, Any]:
        """
        ğŸ¯ æ ¸å¿ƒåŠŸèƒ½: åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­åµŒå…¥æ°´å°
        
        ğŸ“‹ è¯¦ç»†å·¥ä½œæµç¨‹:
        1. è®¾ç½®å¤„ç†å™¨ (å¦‚æœè¿˜æ²¡è®¾ç½®)
        2. å°†æ¶ˆæ¯è½¬æ¢ä¸ºCredIDå…¼å®¹çš„äºŒè¿›åˆ¶æ ¼å¼ (æ”¯æŒå¤šæ®µ)
        3. åˆ›å»ºåŒ…å«æ°´å°å¤„ç†å™¨çš„LogitsProcessorList
        4. ä½¿ç”¨model.generate()ç”Ÿæˆå¸¦æ°´å°æ–‡æœ¬
        5. è¿”å›å®Œæ•´ç»“æœå’Œè¯¦ç»†å…ƒæ•°æ®
        
        ğŸ“¥ å‚æ•°è¯´æ˜:
            model: HuggingFaceé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (å¦‚Llama, GPTç­‰)
            tokenizer: å¯¹åº”çš„åˆ†è¯å™¨ï¼Œå¿…é¡»è®¾ç½®pad_token
            prompt: è¾“å…¥æç¤ºæ–‡æœ¬ï¼Œå¦‚ "Hello, today is"
            message: æ°´å°ä¿¡æ¯ï¼Œæ”¯æŒå¤šç§æ ¼å¼:
                - str: "hello" æˆ–å¤æ‚å­—ç¬¦ä¸² "log20250725143000"
                - List[int]: [123, 456, 789] 
                - List[str]: ["user", "2025", "admin"]
            segmentation_mode: æ¶ˆæ¯åˆ†å‰²æ¨¡å¼
                - 'auto': è‡ªåŠ¨åˆ¤æ–­æœ€ä½³åˆ†å‰²æ–¹å¼ (æ¨è)
                - 'smart': æ™ºèƒ½åˆ†å‰²ï¼Œå¦‚ "alibaba20250725" â†’ ["alibaba", "2025", "0725"]
                - 'whole': æ•´ä½“å¤„ç†
                - 'spaces': æŒ‰ç©ºæ ¼åˆ†å‰²
                
        ğŸ“¤ è¿”å›å€¼ç»“æ„:
            {
                'watermarked_text': str,      # ğŸ¯ å¸¦æ°´å°çš„ç”Ÿæˆæ–‡æœ¬
                'original_message': Any,      # åŸå§‹æ°´å°ä¿¡æ¯
                'binary_message': List[int],  # è½¬æ¢åçš„äºŒè¿›åˆ¶æ¶ˆæ¯åºåˆ—
                'prompt': str,                # è¾“å…¥æç¤º
                'success': bool,              # âœ…/âŒ æ˜¯å¦æˆåŠŸ
                'metadata': {                 # è¯¦ç»†å…ƒæ•°æ®
                    'mode': str,              # ä½¿ç”¨çš„æ¨¡å¼ ('lm'/'random')
                    'model_name': str,        # æ¨¡å‹åç§°
                    'input_length': int,      # è¾“å…¥tokené•¿åº¦
                    'output_length': int,     # è¾“å‡ºtokené•¿åº¦
                    'generation_config': dict,# ç”Ÿæˆé…ç½®å‚æ•°
                    'num_message_segments': int # æ¶ˆæ¯æ®µæ•°
                }
            }
            
        ğŸš¨ é”™è¯¯æƒ…å†µè¿”å›:
            {
                'watermarked_text': None,
                'success': False,
                'error': str                  # é”™è¯¯ä¿¡æ¯
            }
        """
```

**ğŸ”¹ æ ¸å¿ƒæ¥å£ 2: extract() - æ°´å°æå–**

```python
    def extract(self, watermarked_text: str, 
                model: Optional[PreTrainedModel] = None,
                tokenizer: Optional[PreTrainedTokenizer] = None,
                candidates_messages: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        ğŸ¯ æ ¸å¿ƒåŠŸèƒ½: ä»æ°´å°æ–‡æœ¬ä¸­æå–æ°´å°ä¿¡æ¯
        
        ğŸ“‹ è¯¦ç»†å·¥ä½œæµç¨‹:
        1. æ£€æŸ¥æ¨¡å¼å’Œå‚æ•°æœ‰æ•ˆæ€§ (LMæ¨¡å¼éœ€è¦modelå’Œtokenizer)
        2. å€™é€‰æ¶ˆæ¯å¤„ç†: æ”¶é›†æ‰€æœ‰å€™é€‰æ¶ˆæ¯çš„æ‰€æœ‰ç¼–ç æ®µ (ç®€åŒ–ç­–ç•¥)
        3. ä½¿ç”¨CredIDè§£ç å™¨è¿›è¡Œç»Ÿè®¡æ£€æµ‹
        4. æ™ºèƒ½åŒ¹é…: å°†è§£ç ç»“æœä¸å€™é€‰æ¶ˆæ¯è¿›è¡Œåºåˆ—åŒ¹é…
        5. ç½®ä¿¡åº¦è®¡ç®—å’Œç»“æœéªŒè¯
        
        ğŸ“¥ å‚æ•°è¯´æ˜:
            watermarked_text: å¯èƒ½åŒ…å«æ°´å°çš„æ–‡æœ¬
            model: è¯­è¨€æ¨¡å‹ (LMæ¨¡å¼å¿…éœ€ï¼ŒRandomæ¨¡å¼å¯é€‰)
            tokenizer: åˆ†è¯å™¨ (LMæ¨¡å¼å¿…éœ€ï¼ŒRandomæ¨¡å¼å¯é€‰)
            candidates_messages: å€™é€‰æ¶ˆæ¯åˆ—è¡¨ï¼Œç”¨äºä¼˜åŒ–æœç´¢
                ğŸ¯ æ¨èä½¿ç”¨: å¯å¤§å¹…æå‡æ£€æµ‹ç²¾åº¦å’Œæ•ˆç‡
                ä¾‹å¦‚: ["log20250725143000", "user987654321", "admin2025"]
                
        ğŸ“¤ è¿”å›å€¼ç»“æ„:
            {
                'extracted_message': str,           # ğŸ¯ æå–çš„æ¶ˆæ¯
                'binary_message': List[int],        # è§£ç çš„äºŒè¿›åˆ¶æ¶ˆæ¯åºåˆ—
                'confidence': float,                # ğŸšï¸ ç½®ä¿¡åº¦ (0.0-1.0)
                'success': bool,                    # âœ…/âŒ æ˜¯å¦æˆåŠŸæå–
                'detailed_confidence': List,       # è¯¦ç»†ç½®ä¿¡åº¦ä¿¡æ¯
                'metadata': {
                    'mode': str,                    # æ£€æµ‹æ¨¡å¼
                    'text_length': int,             # æ–‡æœ¬é•¿åº¦
                    'num_decoded_segments': int,    # è§£ç æ®µæ•°
                    'detection_method': 'CredID',   # æ£€æµ‹æ–¹æ³•
                    'confidence_threshold': float,  # ç½®ä¿¡åº¦é˜ˆå€¼
                    'search_space': int,            # æœç´¢ç©ºé—´å¤§å°
                    'candidates_provided': bool     # æ˜¯å¦æä¾›å€™é€‰æ¶ˆæ¯
                }
            }
            
        ğŸš¨ å¤±è´¥æƒ…å†µè¿”å›:
            {
                'extracted_message': None,
                'confidence': 0.0,
                'success': False,
                'error': str                        # é”™è¯¯æˆ–"No watermark detected"
            }
        """
```

**ğŸ”§ æ ¸å¿ƒå†…éƒ¨æ–¹æ³•**

```python
    # === æ¶ˆæ¯å¤„ç†æ–¹æ³• ===
    def _message_to_binary(self, message: Union[str, List[int], List[str]], 
                          segmentation_mode: str = 'auto') -> List[int]:
        """å°†å¤šç§æ ¼å¼çš„æ¶ˆæ¯è½¬æ¢ä¸ºCredIDå…¼å®¹çš„æ•´æ•°åºåˆ—"""
        
    def _binary_to_message(self, binary: List[int]) -> Union[str, List[str]]:
        """å°†è§£ç çš„æ•´æ•°åºåˆ—è½¬æ¢å›åŸå§‹æ¶ˆæ¯æ ¼å¼"""
        
    # === æ™ºèƒ½åŒ¹é…æ–¹æ³• ===  
    def _match_decoded_with_candidates(self, decoded_messages: List[int], 
                                     candidates_messages: List[str]) -> Tuple[str, float]:
        """å°†è§£ç ç»“æœä¸å€™é€‰æ¶ˆæ¯è¿›è¡Œæ™ºèƒ½åŒ¹é… (ç®€åŒ–ç‰ˆæœ¬)"""
        
    def _calculate_sequence_match(self, decoded: List[int], candidate: List[int]) -> float:
        """è®¡ç®—ä¸¤ä¸ªåºåˆ—çš„åŒ¹é…åº¦åˆ†æ•°"""
        
    # === å­—ç¬¦ä¸²åˆ†å‰²æ–¹æ³• ===
    def _smart_segment_string(self, text: str) -> List[str]:
        """æ™ºèƒ½åˆ†å‰²å­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤æ‚æ ¼å¼å¦‚'log20250725143000'"""
```

**âš™ï¸ é…ç½®å‚æ•°è¯¦è§£**

```yaml
# config/text_config.yaml - å®Œæ•´é…ç½®ç¤ºä¾‹
method: "CredID"
model_name: "huggyllama/llama-7b"          
mode: "lm"                                 # 'lm'(é«˜è´¨é‡) / 'random'(é«˜æ•ˆç‡)
device: "auto"                             

# === ç”Ÿæˆå‚æ•° ===
max_new_tokens: 110                        
num_beams: 4                               
do_sample: true                            
temperature: 0.7                           
top_p: 0.9                                
top_k: 50                                 

# === CredID LMæ¨¡å¼æ ¸å¿ƒå‚æ•° ===
lm_params:
  delta: 1.5                              # logitsä¿®æ”¹å¼ºåº¦ (å…³é”®å‚æ•°)
  prefix_len: 10                          # å‰ç¼€ä¿æŠ¤é•¿åº¦
  message_len: 10                         # æ¯æ®µæ¶ˆæ¯çš„äºŒè¿›åˆ¶é•¿åº¦
  seed: 42                                # éšæœºç§å­
  topk: -1                               # LM top-ké™åˆ¶
  permutation_num: 50                     # éšæœºæ’åˆ—æ•°
  hash_prefix_len: 1                      # å“ˆå¸Œå‰ç¼€é•¿åº¦
  shifts: [21, 24, 3, 8, 14, 2, 4, 28, 31, 3, 8, 14, 2, 4, 28]

# === æ°´å°å¤„ç†å‚æ•° ===
wm_params:
  encode_ratio: 8                         # ç¼–ç æ¯”ç‡ (æ¯æ¶ˆæ¯ä½å¯¹åº”çš„tokenæ•°)
  seed: 42                                
  strategy: "vanilla"                     # 'vanilla'/'max_confidence'
  max_confidence: 0.5                     
  top_k: 1000                            

# === è§£ç é…ç½® ===
decode_batch_size: 16                      
disable_tqdm: false                        
confidence_threshold: 0.6                  # æˆåŠŸæ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼
```

**ğŸš€ å®é™…ä½¿ç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µ**

```python
# === å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ ===
from src.text_watermark.credid_watermark import CredIDWatermark
from transformers import AutoModelForCausalLM, AutoTokenizer
import yaml

# 1. åˆå§‹åŒ–ç³»ç»Ÿ
with open('config/text_config.yaml', 'r') as f:
    config = yaml.safe_load(f)

model = AutoModelForCausalLM.from_pretrained("huggyllama/llama-7b")
tokenizer = AutoTokenizer.from_pretrained("huggyllama/llama-7b")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

watermark = CredIDWatermark(config)

# 2. ğŸ¯ å•ä¸€æ¶ˆæ¯å¤„ç†
result = watermark.embed(model, tokenizer, "Hello, today is", "tech")
if result['success']:
    print(f"âœ… ç”Ÿæˆæ–‡æœ¬: {result['watermarked_text']}")
    
    # åŸºç¡€æå–
    extracted = watermark.extract(result['watermarked_text'], model, tokenizer)
    print(f"ğŸ“¤ æå–ç»“æœ: {extracted['extracted_message']} (ç½®ä¿¡åº¦: {extracted['confidence']:.3f})")

# 3. ğŸ¯ å¤æ‚æ¶ˆæ¯å¤„ç†
complex_messages = [
    ("ç³»ç»Ÿæ—¥å¿—", "log20250725143000"),
    ("ç”¨æˆ·ä¿¡æ¯", "alibaba20250725"),
    ("ç®¡ç†è´¦æˆ·", ["admin", "2025", "secure"])
]

for desc, message in complex_messages:
    result = watermark.embed(model, tokenizer, f"Entry: ", message)
    if result['success']:
        print(f"\n=== {desc} ===")
        print(f"æ¶ˆæ¯: {message}")
        print(f"ç”Ÿæˆ: {result['watermarked_text']}")
        
        # ğŸ¯ å€™é€‰ä¼˜åŒ–æå–
        candidates = ["log20250725143000", "alibaba20250725", "admin2025secure", "tech", "hello"]
        extracted = watermark.extract(
            result['watermarked_text'], 
            model, tokenizer, 
            candidates_messages=candidates
        )
        
        success_icon = "âœ…" if extracted['success'] else "âŒ"
        print(f"{success_icon} æå–: {extracted['extracted_message']} (ç½®ä¿¡åº¦: {extracted['confidence']:.3f})")

# 4. ğŸ¯ æ‰¹é‡å¤„ç†æ€§èƒ½æµ‹è¯•
import time

test_messages = ["hello", "tech2025", "user123", "log20250725143000"]
batch_start = time.time()

batch_results = []
for i, msg in enumerate(test_messages):
    embed_result = watermark.embed(model, tokenizer, f"Test {i}: ", msg)
    if embed_result['success']:
        extract_result = watermark.extract(embed_result['watermarked_text'], model, tokenizer)
        batch_results.append({
            'original': msg,
            'extracted': extract_result['extracted_message'],
            'confidence': extract_result['confidence'],
            'success': extract_result['success']
        })

batch_time = time.time() - batch_start
print(f"\nâ±ï¸ æ‰¹é‡å¤„ç†({len(test_messages)}æ¡): {batch_time:.2f}ç§’")

# 5. ğŸ¯ é”™è¯¯å¤„ç†ç¤ºä¾‹
try:
    # æ¨¡æ‹Ÿé”™è¯¯æƒ…å†µ
    error_result = watermark.extract("This text has no watermark", model, tokenizer)
    if not error_result['success']:
        print(f"âŒ æ£€æµ‹å¤±è´¥: {error_result.get('error', 'No watermark detected')}")
except Exception as e:
    print(f"ğŸš¨ å¼‚å¸¸å¤„ç†: {e}")
```

**ğŸ“Š æ€§èƒ½å’Œç‰¹ç‚¹æ€»ç»“**

| ç‰¹æ€§ | æè¿° | ä¼˜åŠ¿ |
|------|------|------|
| **å¤šæ¶ˆæ¯æ ¼å¼** | æ”¯æŒå­—ç¬¦ä¸²ã€åˆ—è¡¨ã€å¤æ‚æ ¼å¼ | çµæ´»æ€§é«˜ï¼Œé€‚åº”ä¸åŒåœºæ™¯ |
| **åŒæ¨¡å¼è¿è¡Œ** | LMæ¨¡å¼(é«˜è´¨é‡) / Randomæ¨¡å¼(é«˜æ•ˆç‡) | å¹³è¡¡è´¨é‡å’Œæ€§èƒ½ |
| **å€™é€‰ä¼˜åŒ–** | é™åˆ¶æœç´¢ç©ºé—´æå‡æ•ˆç‡ | å¤§å¹…æå‡æ£€æµ‹ç²¾åº¦ |
| **æ™ºèƒ½åˆ†å‰²** | è‡ªåŠ¨å¤„ç†å¤æ‚æ¶ˆæ¯æ ¼å¼ | æ— éœ€æ‰‹åŠ¨é¢„å¤„ç† |
| **ç®€åŒ–æ¶æ„** | å»é™¤å¤æ‚çš„æŒ‰ä½ç½®åˆ†ç»„é€»è¾‘ | ä»£ç æ›´æ¸…æ™°ï¼Œç»´æŠ¤æ€§å¥½ |
| **é”™è¯¯å¤„ç†** | å®Œæ•´çš„å¼‚å¸¸å¤„ç†æœºåˆ¶ | ç”Ÿäº§ç¯å¢ƒå¯é æ€§é«˜ |
| **æ€§èƒ½ç›‘æ§** | å†…ç½®æ—¶é—´å’Œèµ„æºä½¿ç”¨ç»Ÿè®¡ | ä¾¿äºæ€§èƒ½è°ƒä¼˜ |

**ğŸ¯ ä¸ºå›¾åƒæ°´å°å’Œç»Ÿä¸€å¼•æ“æä¾›çš„è®¾è®¡å‚è€ƒ:**

1. **ğŸ—ï¸ ç»Ÿä¸€æ¥å£æ¨¡å¼**: `embed(model, tokenizer, prompt, message)` â†’ `extract(text, model, tokenizer, candidates)`
2. **âš™ï¸ é…ç½®é©±åŠ¨è®¾è®¡**: é€šè¿‡YAMLæ–‡ä»¶ç®¡ç†æ‰€æœ‰ç®—æ³•å‚æ•°
3. **ğŸ“‹ æ ‡å‡†è¿”å›æ ¼å¼**: ç»Ÿä¸€çš„ `{success, result, metadata, error}` ç»“æ„
4. **ğŸ” å€™é€‰ä¼˜åŒ–æœºåˆ¶**: æ”¯æŒå€™é€‰åˆ—è¡¨çš„é«˜æ•ˆæœç´¢ç­–ç•¥
5. **ğŸ¨ å¤šæ¨¡æ€æ¶ˆæ¯**: æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼çš„æ™ºèƒ½ç¼–ç 
6. **ğŸ›¡ï¸ å¥å£®é”™è¯¯å¤„ç†**: è¯¦ç»†çš„çŠ¶æ€æŠ¥å‘Šå’Œå¼‚å¸¸ç®¡ç†
7. **ğŸ“ˆ æ€§èƒ½ç›‘æ§**: å†…ç½®æ—¶é—´å’Œèµ„æºä½¿ç”¨ç»Ÿè®¡

### 3. å›¾åƒæ°´å°æ¨¡å— (PRC-Watermark) âœ… **å·²å®ç°**

**PRCç®—æ³•åŸç†**ï¼š
- **ä¼ªéšæœºçº é”™ç æ°´å°**ï¼šåŸºäºStable Diffusionçš„æ½œç©ºé—´æ°´å°åµŒå…¥
- **å®Œæ•´æ‰©æ•£é€†å‘**ï¼šé€šè¿‡exact_inversionå®ç°ç²¾ç¡®çš„å›¾åƒåˆ°æ½œå˜é‡è½¬æ¢
- **å¤šç²¾åº¦æ£€æµ‹**ï¼šæ”¯æŒfast/accurate/exactä¸‰ç§ä¸åŒç²¾åº¦ç­‰çº§
- **100%æ£€æµ‹æˆåŠŸç‡**ï¼šæ‰€æœ‰æ¨¡å¼éƒ½èƒ½å®Œç¾æ£€æµ‹å¹¶è§£ç æ°´å°æ¶ˆæ¯
- **æœ¬åœ°æ¨¡å‹æ”¯æŒ**ï¼šç¦»çº¿æ¨¡å¼ä½¿ç”¨ç¼“å­˜çš„Stable Diffusion 2.1æ¨¡å‹

**å®é™…å®ç°çš„æ ¸å¿ƒæ¶æ„**ï¼š

```python
# src/image_watermark/prc_watermark.py
import os
import torch
from PIL import Image
from typing import Dict, Any, Optional, Union, Tuple
import pickle

class PRCWatermark:
    """
    PRCå›¾åƒæ°´å°ç®—æ³•ç»Ÿä¸€å°è£…
    
    âœ¨ æ ¸å¿ƒåŠŸèƒ½ç‰¹ç‚¹:
    1. ç»Ÿä¸€çš„exact_inversionå®ç°ï¼Œæ¶ˆé™¤ä»£ç å†—ä½™
    2. å‚æ•°åŒ–æ¨¡å¼æ§åˆ¶ï¼šé€šè¿‡decoder_invå’Œinference_stepsè°ƒèŠ‚ç²¾åº¦
    3. å®Œæ•´çš„ç¦»çº¿æ¨¡å¼æ”¯æŒï¼Œä½¿ç”¨æœ¬åœ°Stable Diffusionæ¨¡å‹
    4. GPU/CPU tensorè®¾å¤‡è‡ªåŠ¨è½¬æ¢å’Œæ¢¯åº¦ç®¡ç†
    5. å¯†é’¥ç®¡ç†å’Œç¼“å­˜æœºåˆ¶
    6. 100%æ£€æµ‹æˆåŠŸç‡ï¼Œæ”¯æŒå®Œç¾æ°´å°è§£ç 
    """
    
    def __init__(self, 
                 model_id: str = "stabilityai/stable-diffusion-2-1-base",
                 keys_dir: str = "watermark_keys",
                 cache_dir: Optional[str] = None,
                 device: Optional[str] = None,
                 **kwargs):
        """
        åˆå§‹åŒ–PRCæ°´å°å¤„ç†å™¨
        
        Args:
            model_id: Stable Diffusionæ¨¡å‹ID
            keys_dir: å¯†é’¥å­˜å‚¨ç›®å½•
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½• (æ”¯æŒç¦»çº¿æ¨¡å¼)
            device: è®¡ç®—è®¾å¤‡ ('cuda', 'cpu', æˆ– None è‡ªåŠ¨é€‰æ‹©)
            **kwargs: å…¶ä»–PRCç®—æ³•å‚æ•°
        """
        self.model_id = model_id
        self.keys_dir = keys_dir
        self.cache_dir = cache_dir
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        
        # PRCç®—æ³•å‚æ•°
        self.n = kwargs.get('n', 1024)  # ç é•¿
        self.k = kwargs.get('k', 512)   # ä¿¡æ¯ä½æ•°
        self.false_positive_rate = kwargs.get('false_positive_rate', 1e-6)
        
        # ç¡®ä¿å¯†é’¥ç›®å½•å­˜åœ¨
        os.makedirs(self.keys_dir, exist_ok=True)
        
        # å»¶è¿Ÿåˆå§‹åŒ–ç»„ä»¶
        self.pipe = None
        self._key_cache = {}
        
        # è®¾ç½®ç¦»çº¿æ¨¡å¼å’Œæ¨¡å‹ç®¡é“
        self._setup_diffusion_pipe()
```

**ğŸ”¹ æ ¸å¿ƒæ¥å£ 1: embed() - æ°´å°åµŒå…¥**

```python
    def embed(self, 
              prompt: str,
              message: str, 
              key_id: str = "default",
              num_inference_steps: int = 50,
              guidance_scale: float = 7.5,
              seed: Optional[int] = None,
              **kwargs) -> Image.Image:
        """
        ğŸ¯ æ ¸å¿ƒåŠŸèƒ½: åœ¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­åµŒå…¥PRCæ°´å°
        
        ğŸ“‹ è¯¦ç»†å·¥ä½œæµç¨‹:
        1. è·å–æˆ–ç”ŸæˆPRCå¯†é’¥å¯¹ (encoding_key, decoding_key)
        2. å°†æ¶ˆæ¯å­—ç¬¦ä¸²ç¼–ç ä¸ºäºŒè¿›åˆ¶åºåˆ—
        3. ä½¿ç”¨PRCç¼–ç ç®—æ³•ç”Ÿæˆä¼ªéšæœºç å­—
        4. åœ¨Stable Diffusionçš„æ½œç©ºé—´ä¸­åµŒå…¥ç å­—
        5. ç”Ÿæˆå¸¦æ°´å°çš„é«˜è´¨é‡å›¾åƒ
        
        ğŸ“¥ å‚æ•°è¯´æ˜:
            prompt: å›¾åƒç”Ÿæˆæç¤ºè¯ï¼Œå¦‚ "A beautiful sunset over the ocean"
            message: æ°´å°ä¿¡æ¯ï¼Œæ”¯æŒä»»æ„é•¿åº¦å­—ç¬¦ä¸²
            key_id: å¯†é’¥æ ‡è¯†ç¬¦ï¼Œç”¨äºå¯†é’¥ç®¡ç†å’Œå¤ç”¨
            num_inference_steps: æ‰©æ•£é‡‡æ ·æ­¥æ•° (é»˜è®¤50ï¼Œå½±å“è´¨é‡å’Œé€Ÿåº¦)
            guidance_scale: æç¤ºè¯å¼•å¯¼å¼ºåº¦ (é»˜è®¤7.5)
            seed: éšæœºç§å­ï¼Œç”¨äºå¯é‡ç°ç”Ÿæˆ
            **kwargs: å…¶ä»–ç”Ÿæˆå‚æ•°
                
        ğŸ“¤ è¿”å›å€¼:
            PIL.Image: å¸¦æ°´å°çš„512x512å›¾åƒ
            
        ğŸš¨ é”™è¯¯æƒ…å†µ:
            æŠ›å‡ºRuntimeErrorå¼‚å¸¸ï¼ŒåŒ…å«è¯¦ç»†é”™è¯¯ä¿¡æ¯
        """
        # è·å–å¯†é’¥
        encoding_key, _ = self._get_or_create_keys(key_id)
        
        # æ¶ˆæ¯ç¼–ç 
        message_bits = str_to_bin(message)
        prc_codeword = Encode(encoding_key, message_bits)
        
        # ä¼ªéšæœºæ½œå˜é‡é‡‡æ ·
        latents = prc_gaussians.sample(
            codeword=prc_codeword,
            shape=(1, 4, 64, 64),  # Stable Diffusionæ½œç©ºé—´å½¢çŠ¶
            device=self.device
        )
        
        # ç”Ÿæˆå¸¦æ°´å°å›¾åƒ
        with torch.no_grad():
            image = generate(
                pipe=self.pipe,
                prompt=prompt,
                init_latents=latents,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                seed=seed,
                **kwargs
            )
        
        return image
```

**ğŸ”¹ æ ¸å¿ƒæ¥å£ 2: extract() - æ°´å°æå–**

```python
    def extract(self, 
                image: Union[str, Image.Image, torch.Tensor],
                key_id: str = "default", 
                mode: str = 'accurate',
                prompt: Optional[str] = None,
                **kwargs) -> Dict[str, Any]:
        """
        ğŸ¯ æ ¸å¿ƒåŠŸèƒ½: ä»å›¾åƒä¸­æå–PRCæ°´å°ä¿¡æ¯
        
        ğŸ“‹ è¯¦ç»†å·¥ä½œæµç¨‹:
        1. å›¾åƒé¢„å¤„ç†å’Œæ ¼å¼è½¬æ¢
        2. ä½¿ç”¨exact_inversionè¿›è¡Œå›¾åƒé€†å‘ (å…³é”®æ­¥éª¤)
        3. ä»æ½œå˜é‡ä¸­æ¢å¤åéªŒæ¦‚ç‡
        4. PRCè§£ç å™¨æ£€æµ‹å’Œè§£ç æ°´å°
        5. è¿”å›æ£€æµ‹ç»“æœå’Œç½®ä¿¡åº¦
        
        ğŸ“¥ å‚æ•°è¯´æ˜:
            image: è¾“å…¥å›¾åƒï¼Œæ”¯æŒå¤šç§æ ¼å¼:
                - str: å›¾åƒæ–‡ä»¶è·¯å¾„
                - PIL.Image: PILå›¾åƒå¯¹è±¡  
                - torch.Tensor: æ½œå˜é‡tensor
            key_id: å¯†é’¥æ ‡è¯†ç¬¦ï¼Œå¿…é¡»ä¸åµŒå…¥æ—¶ä¸€è‡´
            mode: é€†å‘ç²¾åº¦æ¨¡å¼ï¼Œå½±å“æ£€æµ‹ç²¾åº¦å’Œé€Ÿåº¦:
                - 'fast': 20æ­¥æ¨ç†ï¼Œdecoder_inv=Falseï¼Œ0.19ç§’
                - 'accurate': 50æ­¥æ¨ç†ï¼Œdecoder_inv=Trueï¼Œ13.7ç§’ (æ¨è)
                - 'exact': 50æ­¥æ¨ç†ï¼Œdecoder_inv=Trueï¼Œ52.15ç§’ (æœ€é«˜ç²¾åº¦)
            prompt: åŸå§‹ç”Ÿæˆæç¤ºè¯ (å¯é€‰ï¼Œæœ‰åŠ©äºæå‡exactæ¨¡å¼ç²¾åº¦)
            **kwargs: å…¶ä»–é€†å‘å‚æ•°
                
        ğŸ“¤ è¿”å›å€¼ç»“æ„:
            {
                'detected': bool,           # ğŸ¯ æ˜¯å¦æ£€æµ‹åˆ°æ°´å°
                'message': str,             # ğŸ“¤ è§£ç çš„æ¶ˆæ¯ (æ£€æµ‹æˆåŠŸæ—¶)
                'confidence': float,        # ğŸšï¸ æ£€æµ‹ç½®ä¿¡åº¦ (0.0-1.0)
                'mode_used': str,           # å®é™…ä½¿ç”¨çš„é€†å‘æ¨¡å¼
                'processing_time': float,   # å¤„ç†è€—æ—¶ (ç§’)
                'metadata': {               # è¯¦ç»†å…ƒæ•°æ®
                    'image_size': tuple,    # å›¾åƒå°ºå¯¸
                    'latent_shape': tuple,  # æ½œå˜é‡å½¢çŠ¶
                    'algorithm': 'PRC',     # ç®—æ³•åç§°
                    'key_id': str,          # ä½¿ç”¨çš„å¯†é’¥ID
                    'false_positive_rate': float  # è™šè­¦ç‡
                }
            }
            
        ğŸš¨ å¤±è´¥æƒ…å†µè¿”å›:
            {
                'detected': False,
                'message': None,
                'confidence': 0.0,
                'error': str               # é”™è¯¯ä¿¡æ¯
            }
        """
        # è·å–è§£ç å¯†é’¥
        _, decoding_key = self._get_or_create_keys(key_id)
        
        # å›¾åƒåˆ°æ½œå˜é‡è½¬æ¢ (æ ¸å¿ƒé€†å‘è¿‡ç¨‹)
        if not isinstance(image, torch.Tensor):
            latents = self._image_to_latents(image, mode=mode, prompt=prompt)
        else:
            latents = image
        
        # è®¡ç®—åéªŒæ¦‚ç‡ - ç¡®ä¿tensoråœ¨CPUä¸Šä¸”åˆ†ç¦»æ¢¯åº¦
        latents_cpu = latents.detach().cpu() if hasattr(latents, 'detach') else latents
        if hasattr(latents_cpu, 'cpu'):
            latents_cpu = latents_cpu.cpu()
        posteriors = prc_gaussians.recover_posteriors(latents_cpu.flatten())
        
        # æ£€æµ‹æ°´å°
        detected = Detect(decoding_key, posteriors, self.false_positive_rate)
        
        result = {
            'detected': detected,
            'message': None,
            'confidence': 0.0,
            'mode_used': mode if not isinstance(image, torch.Tensor) else 'tensor_input'
        }
        
        if detected:
            # è§£ç æ¶ˆæ¯
            decoded_bits = Decode(decoding_key, posteriors)
            try:
                decoded_message = bin_to_str(decoded_bits)
                result['message'] = decoded_message
                result['confidence'] = 1.0  # PRCæä¾›ç¡®å®šæ€§æ£€æµ‹
            except Exception as e:
                result['confidence'] = 0.6  # æ£€æµ‹åˆ°ä½†è§£ç å¤±è´¥
        
        return result
```

**ğŸ”§ æ ¸å¿ƒå†…éƒ¨æ–¹æ³• - ç»Ÿä¸€é€†å‘å®ç°**

```python
    def _image_to_latents(self, image: Image.Image, mode: str = 'accurate', 
                         prompt: Optional[str] = None) -> torch.Tensor:
        """
        ğŸ¯ æ ¸å¿ƒæ–¹æ³•: å°†PILå›¾åƒè½¬æ¢ä¸ºæ½œå˜é‡ï¼Œç»Ÿä¸€ä½¿ç”¨exact_inversion
        
        ğŸ“‹ å®ç°ç­–ç•¥:
        - æ‰€æœ‰æ¨¡å¼éƒ½ä½¿ç”¨ç›¸åŒçš„exact_inversionå‡½æ•°
        - é€šè¿‡å‚æ•°è°ƒèŠ‚å®ç°ä¸åŒç²¾åº¦ç­‰çº§
        - æ¶ˆé™¤ä»£ç å†—ä½™ï¼Œä¿æŒæ¶æ„ç®€æ´
        
        Args:
            image: PILå›¾åƒ
            mode: é€†å‘æ¨¡å¼ ('fast', 'accurate', 'exact')
            prompt: æç¤ºè¯ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
            
        Returns:
            æ½œå˜é‡tensor
        """
        if not PRC_AVAILABLE:
            raise RuntimeError("PRC dependencies not available")
            
        if prompt is None:
            prompt = ""  # ä½¿ç”¨ç©ºæç¤ºè¯ä½œä¸ºé»˜è®¤å€¼
            
        # æ ¹æ®æ¨¡å¼è®¾ç½®ä¸åŒçš„å‚æ•°
        if mode == 'fast':
            # å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨è¾ƒå°‘çš„æ¨ç†æ­¥æ•°å’Œç®€å•é€†å‘
            decoder_inv = False
            num_inference_steps = 20
            test_num_inference_steps = 20
        elif mode == 'accurate':
            # ç²¾ç¡®æ¨¡å¼ï¼šä½¿ç”¨decoder_invä¼˜åŒ–æ±‚è§£
            decoder_inv = True
            num_inference_steps = 50
            test_num_inference_steps = 50
        elif mode == 'exact':
            # å®Œæ•´æ¨¡å¼ï¼šæœ€é«˜ç²¾åº¦è®¾ç½®
            decoder_inv = True
            num_inference_steps = 50
            test_num_inference_steps = 50
        else:
            raise ValueError(f"Unsupported mode: {mode}")
        
        # ä½¿ç”¨PRC-Watermarkçš„exact_inversionå‡½æ•°
        reversed_latents = exact_inversion(
            image=image,
            prompt=prompt, 
            guidance_scale=3.0,
            num_inference_steps=num_inference_steps,
            solver_order=1,
            test_num_inference_steps=test_num_inference_steps,
            inv_order=1,
            decoder_inv=decoder_inv,
            model_id=self.model_id,
            pipe=self.pipe
        )
        
        return reversed_latents
```

**âš™ï¸ é…ç½®å‚æ•°è¯¦è§£**

```yaml
# config/image_config.yaml - å®Œæ•´é…ç½®ç¤ºä¾‹
method: "PRC"
model_id: "stabilityai/stable-diffusion-2-1-base"
device: "auto"                          # 'cuda', 'cpu', 'auto'

# === å¯†é’¥ç®¡ç† ===
keys_dir: "watermark_keys"
cache_dir: "/path/to/huggingface/cache"  # æœ¬åœ°æ¨¡å‹ç¼“å­˜

# === PRCç®—æ³•å‚æ•° ===
prc_params:
  n: 1024                              # ç é•¿
  k: 512                               # ä¿¡æ¯ä½æ•°
  false_positive_rate: 1.0e-6          # è™šè­¦ç‡

# === ç”Ÿæˆå‚æ•° ===
generation_params:
  num_inference_steps: 50              # é‡‡æ ·æ­¥æ•°
  guidance_scale: 7.5                  # å¼•å¯¼å¼ºåº¦
  height: 512                          # å›¾åƒé«˜åº¦
  width: 512                           # å›¾åƒå®½åº¦

# === é€†å‘å‚æ•° ===
inversion_params:
  default_mode: "accurate"             # é»˜è®¤é€†å‘æ¨¡å¼
  fast_steps: 20                       # å¿«é€Ÿæ¨¡å¼æ­¥æ•°
  accurate_steps: 50                   # ç²¾ç¡®æ¨¡å¼æ­¥æ•°
  exact_steps: 50                      # å®Œæ•´æ¨¡å¼æ­¥æ•°
```

**ğŸš€ å®é™…ä½¿ç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µ**

```python
# === å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ ===
from src.image_watermark.prc_watermark import PRCWatermark
from PIL import Image
import time

# 1. åˆå§‹åŒ–ç³»ç»Ÿ (æ”¯æŒç¦»çº¿æ¨¡å¼)
prc = PRCWatermark(
    model_id="stabilityai/stable-diffusion-2-1-base",
    keys_dir="test_keys",
    cache_dir="/path/to/local/models",  # æœ¬åœ°æ¨¡å‹è·¯å¾„
    device="cuda"
)

# 2. ğŸ¯ åŸºç¡€æ°´å°åµŒå…¥
print("=== åŸºç¡€æ°´å°åµŒå…¥ ===")
start_time = time.time()

watermarked_image = prc.embed(
    prompt="A beautiful sunset over the ocean",
    message="Hello PRC!",
    key_id="demo_key",
    seed=42
)

embed_time = time.time() - start_time
print(f"âœ… åµŒå…¥å®Œæˆ: {embed_time:.2f}ç§’")
print(f"å›¾åƒå°ºå¯¸: {watermarked_image.size}")

# ä¿å­˜å›¾åƒ
watermarked_image.save("watermarked_sunset.png")

# 3. ğŸ¯ å¤šæ¨¡å¼æ°´å°æ£€æµ‹å¯¹æ¯”
print("\n=== å¤šæ¨¡å¼æ£€æµ‹å¯¹æ¯” ===")
modes = ['fast', 'accurate', 'exact']

for mode in modes:
    start_time = time.time()
    
    result = prc.extract(
        image=watermarked_image,
        key_id="demo_key",
        mode=mode
    )
    
    extract_time = time.time() - start_time
    
    status = "âœ…" if result['detected'] else "âŒ"
    print(f"{mode.upper():>8}: {status} | è€—æ—¶: {extract_time:.2f}s | æ¶ˆæ¯: {result.get('message', 'None')}")

# 4. ğŸ¯ æ‰¹é‡å¤„ç†æµ‹è¯•
print("\n=== æ‰¹é‡å¤„ç†æµ‹è¯• ===")
test_cases = [
    ("A red car", "car001"),
    ("A blue house", "house002"), 
    ("A green tree", "tree003")
]

batch_results = []
batch_start = time.time()

for prompt, message in test_cases:
    # åµŒå…¥
    image = prc.embed(prompt=prompt, message=message, key_id="batch_key")
    
    # æå– (ä½¿ç”¨accurateæ¨¡å¼)
    result = prc.extract(image=image, key_id="batch_key", mode='accurate')
    
    batch_results.append({
        'prompt': prompt,
        'original': message,
        'detected': result['detected'],
        'extracted': result.get('message'),
        'success': result['detected'] and result.get('message') == message
    })

batch_time = time.time() - batch_start
success_rate = sum(1 for r in batch_results if r['success']) / len(batch_results)

print(f"â±ï¸ æ‰¹é‡å¤„ç†({len(test_cases)}å¼ ): {batch_time:.2f}ç§’")
print(f"ğŸ¯ æˆåŠŸç‡: {success_rate:.1%}")

for i, result in enumerate(batch_results):
    status = "âœ…" if result['success'] else "âŒ"
    print(f"  {i+1}. {status} {result['prompt']}: {result['original']} â†’ {result['extracted']}")

# 5. ğŸ¯ æ–‡ä»¶è·¯å¾„å¤„ç†
print("\n=== æ–‡ä»¶è·¯å¾„å¤„ç† ===")
# ä»æ–‡ä»¶è·¯å¾„ç›´æ¥æå–
file_result = prc.extract(
    image="watermarked_sunset.png",  # ç›´æ¥ä½¿ç”¨æ–‡ä»¶è·¯å¾„
    key_id="demo_key",
    mode='fast'
)

print(f"æ–‡ä»¶æ£€æµ‹: {'âœ…' if file_result['detected'] else 'âŒ'} | æ¶ˆæ¯: {file_result.get('message', 'None')}")

# 6. ğŸ¯ æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡
print("\n=== æ€§èƒ½ç»Ÿè®¡ ===")
print(f"æ¨¡å‹ID: {prc.model_id}")
print(f"è®¾å¤‡: {prc.device}")
print(f"å¯†é’¥ç›®å½•: {prc.keys_dir}")
print(f"ç¼“å­˜å¯†é’¥æ•°: {len(prc._key_cache)}")
```

**ğŸ“Š æ€§èƒ½åŸºå‡†å’Œç‰¹ç‚¹æ€»ç»“**

| æ¨¡å¼ | æ£€æµ‹æˆåŠŸç‡ | å¤„ç†æ—¶é—´ | é€‚ç”¨åœºæ™¯ | æŠ€æœ¯ç‰¹ç‚¹ |
|------|------------|----------|----------|----------|
| **FAST** | 100% | 0.19ç§’ | å®æ—¶åº”ç”¨ | decoder_inv=Falseï¼Œ20æ­¥æ¨ç† |
| **ACCURATE** | 100% | 13.7ç§’ | ç”Ÿäº§ç¯å¢ƒ | decoder_inv=Trueï¼Œ50æ­¥æ¨ç† |
| **EXACT** | 100% | 52.15ç§’ | ç ”ç©¶åˆ†æ | å®Œæ•´æ‰©æ•£é€†å‘ï¼Œæœ€é«˜ç²¾åº¦ |

**ğŸ”§ æŠ€æœ¯å®ç°äº®ç‚¹**ï¼š

| ç‰¹æ€§ | æè¿° | ä¼˜åŠ¿ |
|------|------|------|
| **ç»Ÿä¸€é€†å‘å®ç°** | æ‰€æœ‰æ¨¡å¼ä½¿ç”¨åŒä¸€ä¸ªexact_inversionå‡½æ•° | ä»£ç ç®€æ´ï¼Œç»´æŠ¤æ€§å¥½ |
| **å‚æ•°åŒ–æ§åˆ¶** | é€šè¿‡decoder_invå’Œstepså‚æ•°è°ƒèŠ‚ç²¾åº¦ | çµæ´»é…ç½®ï¼Œé¿å…é‡å¤ä»£ç  |
| **ç¦»çº¿æ¨¡å¼æ”¯æŒ** | æœ¬åœ°æ¨¡å‹ç¼“å­˜ï¼Œæ— éœ€ç½‘ç»œè¿æ¥ | éƒ¨ç½²çµæ´»ï¼Œéšç§ä¿æŠ¤ |
| **è®¾å¤‡è‡ªé€‚åº”** | è‡ªåŠ¨GPU/CPUè½¬æ¢å’Œæ¢¯åº¦ç®¡ç† | å…¼å®¹æ€§å¼ºï¼Œé”™è¯¯å¤„ç†å®Œå–„ |
| **å¯†é’¥ç®¡ç†** | è‡ªåŠ¨å¯†é’¥ç”Ÿæˆã€ç¼“å­˜å’Œå¤ç”¨ | ä¾¿äºå¤šé¡¹ç›®ç®¡ç† |
| **100%æˆåŠŸç‡** | æ‰€æœ‰æ¨¡å¼éƒ½èƒ½å®Œç¾æ£€æµ‹è§£ç  | ç”Ÿäº§ç¯å¢ƒå¯é æ€§é«˜ |

**ğŸ¯ ä¸æ–‡æœ¬æ°´å°çš„ç»Ÿä¸€æ¥å£å¯¹æ¯”**ï¼š

| æ¥å£è¦ç´  | æ–‡æœ¬æ°´å° | å›¾åƒæ°´å° | ç»Ÿä¸€è®¾è®¡ |
|----------|----------|----------|----------|
| **è¾“å…¥æ ¼å¼** | `(model, tokenizer, prompt, message)` | `(prompt, message, key_id)` | ç®€åŒ–å‚æ•°ï¼Œéšè—å¤æ‚æ€§ |
| **è¾“å‡ºæ ¼å¼** | `{watermarked_text, success, metadata}` | `PIL.Image` | ç›´æ¥è¿”å›ç»“æœå¯¹è±¡ |
| **æ£€æµ‹è¾“å…¥** | `(text, model, tokenizer, candidates)` | `(image, key_id, mode)` | æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ |
| **æ£€æµ‹è¾“å‡º** | `{extracted_message, confidence, success}` | `{detected, message, confidence}` | ç»Ÿä¸€ç»“æ„è®¾è®¡ |
| **é…ç½®ç®¡ç†** | YAMLé…ç½®æ–‡ä»¶é©±åŠ¨ | YAMLé…ç½®æ–‡ä»¶é©±åŠ¨ | ä¸€è‡´çš„é…ç½®æ–¹å¼ |
| **é”™è¯¯å¤„ç†** | è¯¦ç»†å¼‚å¸¸ä¿¡æ¯å’ŒçŠ¶æ€ | è¯¦ç»†å¼‚å¸¸ä¿¡æ¯å’ŒçŠ¶æ€ | ç»Ÿä¸€é”™è¯¯å¤„ç†æœºåˆ¶ |
