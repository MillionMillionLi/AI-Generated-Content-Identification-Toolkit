å¥½çš„ï¼æŒ‰ç…§è¿™ä¸ªç›®å½•ç»“æ„é‡æ–°åˆ¶å®šåå¤©è®¡åˆ’ã€‚è¿™ä¸ªç»“æ„æ›´æœ‰æ¡ç†ï¼Œæˆ‘æ¥ä¸ºä½ è®¾è®¡ä¸€ä¸ªé€‚åˆçš„å¼€å‘è®¡åˆ’ã€‚

å¥½çš„ï¼æ ¹æ®ä½ é€‰æ‹©çš„æ ‡å‡†ç›®å½•ç»“æ„ï¼Œæˆ‘æ¥é‡æ–°åˆ¶å®šä¸€ä¸ªæ›´åˆç†çš„åå¤©å¼€å‘è®¡åˆ’ã€‚
Day 1: é¡¹ç›®æ­å»º - åˆ›å»ºæç®€ç›®å½•ç»“æ„ï¼Œè®¾ç½®requirements.txtï¼Œåˆå§‹åŒ–åŸºæœ¬æ–‡ä»¶
Day 2: å¤åˆ¶CredIDä»£ç  - ä»åŸé¡¹ç›®å¤åˆ¶æ ¸å¿ƒæ–‡ä»¶ï¼Œä¿®å¤å¯¼å…¥è·¯å¾„é—®é¢˜
Day 3: å¤åˆ¶Stable Signatureä»£ç  - å¤åˆ¶æ ¸å¿ƒæ–‡ä»¶ï¼Œä¸‹è½½é¢„è®­ç»ƒè§£ç å™¨
Day 4-5: æ–‡æœ¬æ°´å°å°è£… - åˆ›å»ºTextWatermarkç±»ï¼Œç¡¬ç¼–ç é…ç½®ï¼Œå®ç°åµŒå…¥å’Œæå–æ¥å£
Day 6-7: å›¾åƒæ°´å°å°è£… - åˆ›å»ºImageWatermarkç±»ï¼Œå®ç°å›¾åƒç”Ÿæˆå’Œæ°´å°æå–
Day 8: ç»Ÿä¸€å·¥å…·ç±» - åˆ›å»ºWatermarkToolç±»ï¼Œæ•´åˆæ–‡æœ¬å’Œå›¾åƒæ°´å°åŠŸèƒ½
Day 9: ç¼–å†™demo - åˆ›å»ºunified_demo.pyï¼Œtext_demo.pyï¼Œimage_demo.py


## ğŸ“… åå¤©å¼€å‘è®¡åˆ’ - æ ‡å‡†ç‰ˆ

### ğŸ—ï¸ Day 1-2: é¡¹ç›®åŸºç¡€è®¾æ–½

#### Day 1: ç›®å½•ç»“æ„å’ŒåŸºç¡€å·¥å…·
**ä»»åŠ¡**:
```bash
# 1. åˆ›å»ºé¡¹ç›®ç»“æ„
mkdir mmwt && cd mmwt
mkdir -p {config,src/{text_watermark,image_watermark,utils},examples,tests,models}

# 2. åˆ›å»º__init__.pyæ–‡ä»¶
find src -type d -exec touch {}/__init__.py \;
touch __init__.py

# 3. åŸºç¡€æ–‡ä»¶
touch {README.md,requirements.txt,setup.py}
touch config/{text_config.yaml,image_config.yaml}
touch src/watermark_engine.py
```

**requirements.txt**:
```
torch>=1.12.0
transformers>=4.19.2
diffusers>=0.20.0
numpy>=1.20.0
scipy>=1.7.0
Pillow>=8.0.0
pyyaml>=5.4.0
tqdm>=4.65.0
omegaconf>=2.1.1
scikit-image>=0.20.0
```

#### Day 2: åŸºç¡€å·¥å…·æ¨¡å—
å®ç° `src/utils/config_loader.py` å’Œ `src/utils/model_manager.py`

```python
# src/utils/config_loader.py
import yaml
import os
from typing import Dict, Any

def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Config file not found: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config
```

---

### ğŸ“ Day 3-5: CredIDæ–‡æœ¬æ°´å°é›†æˆ âœ… **å·²å®Œæˆ**

#### Day 3: CredIDä»£ç å¤åˆ¶å’Œä¿®å¤ âœ…
**å·²å®Œæˆä»»åŠ¡:**
```bash
# 1. å¤åˆ¶CredIDæ ¸å¿ƒæ–‡ä»¶åˆ° src/text_watermark/credid/
# ä» credible_LLM_watermarking é¡¹ç›®å¤åˆ¶ä»¥ä¸‹æ–‡ä»¶:
# - watermarking/CredID/ (å®Œæ•´ç›®å½•)
# - src/utils/ (å·¥å…·å‡½æ•°)

# 2. ä¿®å¤å¯¼å…¥è·¯å¾„é—®é¢˜
# è°ƒæ•´æ‰€æœ‰ç›¸å¯¹å¯¼å…¥è·¯å¾„ï¼šfrom src.utils.* â†’ from ...src.utils.*
```

**æ ¸å¿ƒæ–‡ä»¶ç»“æ„:**
```
src/text_watermark/credid/
â”œâ”€â”€ __init__.py                     # åŒ…åˆå§‹åŒ–ï¼Œæš´éœ²æ ¸å¿ƒç»„ä»¶
â”œâ”€â”€ watermarking/CredID/
â”‚   â”œâ”€â”€ message_model_processor.py  # æ ¸å¿ƒå¤„ç†å™¨ (WmProcessorMessageModel)
â”‚   â”œâ”€â”€ random_message_model_processor.py  # éšæœºå¤„ç†å™¨
â”‚   â”œâ”€â”€ base_processor.py          # åŸºç¡€æ¥å£
â”‚   â”œâ”€â”€ random_processor.py        # ç®€å•éšæœºå¤„ç†å™¨
â”‚   â””â”€â”€ message_models/
â”‚       â”œâ”€â”€ lm_message_model.py    # LMæ¶ˆæ¯æ¨¡å‹ (LMMessageModel)
â”‚       â”œâ”€â”€ random_message_model.py # éšæœºæ¶ˆæ¯æ¨¡å‹
â”‚       â”œâ”€â”€ base_message_model.py  # åŸºç¡€æ¶ˆæ¯æ¨¡å‹
â”‚       â””â”€â”€ base_message_model_fast.py
â””â”€â”€ src/utils/                     # CredIDå·¥å…·å‡½æ•°
    â”œâ”€â”€ hash_fn.py                 # å“ˆå¸Œå‡½æ•°
    â””â”€â”€ random_utils.py            # éšæœºå·¥å…·
```

#### Day 4: CredIDå°è£…ç±»å¼€å‘ âœ…
**æ€è€ƒï¼šåµŒå…¥æ°´å°é•¿åº¦çš„é—®é¢˜ï¼Œè¿‡çŸ­ä¼šæ€ä¹ˆæ ·ï¼Œä¸ºä»€ä¹ˆä¹‹å‰çš„æµ‹è¯•åªåµŒå…¥ä¸€ä¸ªä¹Ÿèƒ½æ­£å¸¸è¿è¡Œï¼›å¦‚æœå¤šé€€å°‘è¡¥ï¼Œextractçš„æ—¶å€™æ€ä¹ˆåŠ**
**å®ç°çš„å®Œæ•´æ¥å£:** `src/text_watermark/credid_watermark.py`

```python
class CredIDWatermark:
    """
    CredIDæ–‡æœ¬æ°´å°ç®—æ³•ç»Ÿä¸€å°è£…
    
    åŠŸèƒ½ç‰¹ç‚¹:
    - æ”¯æŒå¤šç§æ¶ˆæ¯æ ¼å¼ (å­—ç¬¦ä¸²ã€æ•´æ•°åˆ—è¡¨ã€å­—ç¬¦ä¸²åˆ—è¡¨)
    - æ”¯æŒä¸¤ç§æ¨¡å¼: LMæ¨¡å¼(é«˜è´¨é‡) / Randomæ¨¡å¼(é«˜é€Ÿåº¦)
    - æ™ºèƒ½å¤šæ®µæ¶ˆæ¯å¤„ç†å’Œå€™é€‰æ¶ˆæ¯ä¼˜åŒ–æœç´¢
    - å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œç½®ä¿¡åº¦è¯„ä¼°
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–CredIDæ°´å°å¤„ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ¨¡å‹å’Œç®—æ³•å‚æ•°
                - mode: 'lm' æˆ– 'random' (é»˜è®¤'lm')
                - model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
                - lm_params: LMæ¨¡å¼å‚æ•° (delta, prefix_len, message_lenç­‰)
                - wm_params: æ°´å°å¤„ç†å‚æ•° (encode_ratio, strategyç­‰)
        """
    
    def embed(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, 
              prompt: str, message: Union[str, List[int], List[str]], 
              segmentation_mode: str = 'auto') -> Dict[str, Any]:
        """
        ğŸ”¹ æ ¸å¿ƒåŠŸèƒ½: åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­åµŒå…¥æ°´å°
        
        å·¥ä½œæµç¨‹:
        1. å°†æ¶ˆæ¯è½¬æ¢ä¸ºCredIDå…¼å®¹çš„äºŒè¿›åˆ¶æ ¼å¼ (æ”¯æŒå¤šæ®µ)
        2. è®¾ç½®LogitsProcessorListï¼Œé›†æˆæ°´å°å¤„ç†å™¨
        3. ä½¿ç”¨model.generate()ç”Ÿæˆå¸¦æ°´å°æ–‡æœ¬
        4. è¿”å›å®Œæ•´ç»“æœå’Œå…ƒæ•°æ®
        
        Args:
            model: HuggingFaceè¯­è¨€æ¨¡å‹
            tokenizer: å¯¹åº”çš„åˆ†è¯å™¨  
            prompt: è¾“å…¥æç¤ºæ–‡æœ¬
            message: æ°´å°ä¿¡æ¯ï¼Œæ”¯æŒ:
                - str: "hello" æˆ– "log20250725143000"
                - List[int]: [123, 456, 789]
                - List[str]: ["user", "2025", "admin"]
            segmentation_mode: æ¶ˆæ¯åˆ†å‰²æ¨¡å¼
                - 'auto': è‡ªåŠ¨åˆ¤æ–­ (é»˜è®¤)
                - 'smart': æ™ºèƒ½åˆ†å‰² (å¦‚ "alibaba20250725" â†’ ["alibaba", "2025", "0725"])
                - 'whole': æ•´ä½“å¤„ç†
                - 'spaces': æŒ‰ç©ºæ ¼åˆ†å‰²
                
        Returns:
            {
                'watermarked_text': str,      # å¸¦æ°´å°çš„ç”Ÿæˆæ–‡æœ¬
                'original_message': Any,      # åŸå§‹æ°´å°ä¿¡æ¯
                'binary_message': List[int],  # è½¬æ¢åçš„äºŒè¿›åˆ¶æ¶ˆæ¯
                'prompt': str,                # è¾“å…¥æç¤º
                'success': bool,              # æ˜¯å¦æˆåŠŸ
                'metadata': {                 # ç”Ÿæˆå…ƒæ•°æ®
                    'mode': str,              # ä½¿ç”¨çš„æ¨¡å¼ ('lm'/'random')
                    'model_name': str,        # æ¨¡å‹åç§°
                    'input_length': int,      # è¾“å…¥é•¿åº¦
                    'output_length': int,     # è¾“å‡ºé•¿åº¦
                    'generation_config': dict # ç”Ÿæˆé…ç½®
                }
            }
        """
    
    def extract(self, watermarked_text: str, 
                model: Optional[PreTrainedModel] = None,
                tokenizer: Optional[PreTrainedTokenizer] = None,
                candidates_messages: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        ğŸ”¹ æ ¸å¿ƒåŠŸèƒ½: ä»æ°´å°æ–‡æœ¬ä¸­æå–æ°´å°ä¿¡æ¯
        
        å·¥ä½œæµç¨‹:
        1. æ£€æŸ¥æ¨¡å¼å’Œå‚æ•°æœ‰æ•ˆæ€§
        2. æ”¶é›†å€™é€‰æ¶ˆæ¯çš„æ‰€æœ‰ç¼–ç  (å¦‚æœæä¾›candidates_messages)
        3. ä½¿ç”¨CredIDè§£ç å™¨è¿›è¡Œç»Ÿè®¡æ£€æµ‹
        4. æ™ºèƒ½åŒ¹é…å€™é€‰æ¶ˆæ¯æˆ–é‡æ„åŸå§‹æ¶ˆæ¯
        5. è®¡ç®—ç½®ä¿¡åº¦å¹¶è¿”å›ç»“æœ
        
        Args:
            watermarked_text: å¯èƒ½åŒ…å«æ°´å°çš„æ–‡æœ¬
            model: è¯­è¨€æ¨¡å‹ (LMæ¨¡å¼å¿…éœ€ï¼ŒRandomæ¨¡å¼å¯é€‰)
            tokenizer: åˆ†è¯å™¨ (LMæ¨¡å¼å¿…éœ€ï¼ŒRandomæ¨¡å¼å¯é€‰)
            candidates_messages: å€™é€‰æ¶ˆæ¯åˆ—è¡¨ï¼Œç”¨äºä¼˜åŒ–æœç´¢
                ä¾‹å¦‚: ["log20250725143000", "user987654321", "admin2025"]
                
        Returns:
            {
                'extracted_message': str,           # æå–çš„æ¶ˆæ¯
                'binary_message': List[int],        # è§£ç çš„äºŒè¿›åˆ¶æ¶ˆæ¯
                'confidence': float,                # ç½®ä¿¡åº¦ (0.0-1.0)
                'success': bool,                    # æ˜¯å¦æˆåŠŸæå–
                'detailed_confidence': List,       # è¯¦ç»†ç½®ä¿¡åº¦ä¿¡æ¯
                'metadata': {
                    'mode': str,                    # æ£€æµ‹æ¨¡å¼
                    'text_length': int,             # æ–‡æœ¬é•¿åº¦
                    'num_decoded_segments': int,    # è§£ç æ®µæ•°
                    'detection_method': 'CredID',   # æ£€æµ‹æ–¹æ³•
                    'confidence_threshold': float,  # ç½®ä¿¡åº¦é˜ˆå€¼
                    'search_space': int,            # æœç´¢ç©ºé—´å¤§å°
                    'candidates_provided': bool     # æ˜¯å¦æä¾›å€™é€‰æ¶ˆæ¯
                }
            }
        """
    
    # å†…éƒ¨æ–¹æ³•
    def _message_to_binary(self, message, segmentation_mode='auto') -> List[int]
    def _binary_to_message(self, binary: List[int]) -> str
    def _match_decoded_with_candidates(self, decoded_messages, candidates) -> Tuple[str, float]
    def _calculate_sequence_match(self, decoded, candidate) -> float
```

#### Day 5: é…ç½®ç³»ç»Ÿå’Œæµ‹è¯• âœ…
**é…ç½®æ–‡ä»¶:** `config/text_config.yaml`
```yaml
# === åŸºç¡€é…ç½® ===
method: "CredID"                          # æ°´å°ç®—æ³•
model_name: "huggyllama/llama-7b"         # è¯­è¨€æ¨¡å‹
mode: "lm"                                # æ¨¡å¼: 'lm'(é«˜è´¨é‡) / 'random'(é«˜é€Ÿåº¦)
device: "auto"                            # è®¾å¤‡: 'auto'/'cuda'/'cpu'

# === ç”Ÿæˆå‚æ•° ===
max_new_tokens: 110                       # æœ€å¤§ç”Ÿæˆtokenæ•°
num_beams: 4                              # beam searchå®½åº¦
do_sample: true                           # æ˜¯å¦é‡‡æ ·
temperature: 0.7                          # ç”Ÿæˆæ¸©åº¦
top_p: 0.9                               # nucleusé‡‡æ ·
top_k: 50                                # top-ké‡‡æ ·

# === CredID LMæ¨¡å¼å‚æ•° ===
lm_params:
  delta: 1.5                              # logitsä¿®æ”¹å¼ºåº¦
  prefix_len: 10                          # å‰ç¼€ä¿æŠ¤é•¿åº¦
  message_len: 10                         # æ¶ˆæ¯äºŒè¿›åˆ¶é•¿åº¦ (ä½)
  seed: 42                                # éšæœºç§å­
  topk: -1                               # LM top-ké™åˆ¶
  permutation_num: 50                     # éšæœºæ’åˆ—æ•°
  hash_prefix_len: 1                      # å“ˆå¸Œå‰ç¼€é•¿åº¦
  shifts: [21, 24, 3, 8, 14, 2, 4, 28, 31, 3, 8, 14, 2, 4, 28]  # å“ˆå¸Œç§»ä½

# === æ°´å°å¤„ç†å‚æ•° ===
wm_params:
  encode_ratio: 8                         # ç¼–ç æ¯”ç‡ (æ¯æ¶ˆæ¯ä½å¯¹åº”çš„tokenæ•°)
  seed: 42                                # æ°´å°ç§å­
  strategy: "vanilla"                     # ç­–ç•¥: 'vanilla'/'max_confidence'
  max_confidence: 0.5                     # æœ€å¤§ç½®ä¿¡åº¦é˜ˆå€¼
  top_k: 1000                            # å¤„ç†å™¨top-k

# === è§£ç é…ç½® ===
decode_batch_size: 16                     # è§£ç æ‰¹æ¬¡å¤§å°
disable_tqdm: false                       # æ˜¯å¦ç¦ç”¨è¿›åº¦æ¡
confidence_threshold: 0.6                 # æˆåŠŸæ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼
```

**ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•:**
```python
# === åŸºç¡€ä½¿ç”¨ç¤ºä¾‹ ===
from src.text_watermark.credid_watermark import CredIDWatermark
from transformers import AutoModelForCausalLM, AutoTokenizer
import yaml

# 1. åŠ è½½é…ç½®å’Œæ¨¡å‹
with open('config/text_config.yaml', 'r') as f:
    config = yaml.safe_load(f)

model = AutoModelForCausalLM.from_pretrained("huggyllama/llama-7b")
tokenizer = AutoTokenizer.from_pretrained("huggyllama/llama-7b")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# 2. åˆå§‹åŒ–æ°´å°
watermark = CredIDWatermark(config)

# 3. åµŒå…¥æ°´å° - æ”¯æŒå¤šç§æ¶ˆæ¯æ ¼å¼
results = []

# ç®€å•å­—ç¬¦ä¸²
result1 = watermark.embed(model, tokenizer, "Hello, today is", "tech")
results.append(("ç®€å•æ¶ˆæ¯", result1))

# å¤æ‚å­—ç¬¦ä¸² (è‡ªåŠ¨æ™ºèƒ½åˆ†å‰²)
result2 = watermark.embed(model, tokenizer, "System log entry:", "log20250725143000")
results.append(("å¤æ‚æ¶ˆæ¯", result2))

# åˆ—è¡¨æ¶ˆæ¯
result3 = watermark.embed(model, tokenizer, "User information:", ["admin", "2025", "secure"])
results.append(("åˆ—è¡¨æ¶ˆæ¯", result3))

# 4. æå–æ°´å°
for desc, result in results:
    if result['success']:
        print(f"\n=== {desc} ===")
        print(f"ç”Ÿæˆæ–‡æœ¬: {result['watermarked_text']}")
        
        # åŸºç¡€æå–
        extracted = watermark.extract(result['watermarked_text'], model, tokenizer)
        print(f"æå–ç»“æœ: {extracted['extracted_message']}")
        print(f"ç½®ä¿¡åº¦: {extracted['confidence']:.3f}")
        print(f"æˆåŠŸ: {extracted['success']}")
        
        # å€™é€‰æ¶ˆæ¯ä¼˜åŒ–æå– (å¯é€‰)
        candidates = ["tech", "log20250725143000", "admin2025secure", "hello", "test"]
        extracted_opt = watermark.extract(
            result['watermarked_text'], 
            model, tokenizer, 
            candidates_messages=candidates
        )
        print(f"ä¼˜åŒ–æå–: {extracted_opt['extracted_message']} (ç½®ä¿¡åº¦: {extracted_opt['confidence']:.3f})")

# === é«˜çº§åŠŸèƒ½ç¤ºä¾‹ ===
# 1. æ‰¹é‡å¤„ç†
messages = ["hello", "tech2025", "user123admin", "log20250725143000"]
prompts = ["Today is", "System:", "User login:", "Entry log:"]

batch_results = []
for prompt, message in zip(prompts, messages):
    result = watermark.embed(model, tokenizer, prompt, message)
    if result['success']:
        extracted = watermark.extract(result['watermarked_text'], model, tokenizer)
        batch_results.append({
            'original': message,
            'extracted': extracted['extracted_message'],
            'confidence': extracted['confidence'],
            'match': message == extracted['extracted_message']
        })

print(f"\n=== æ‰¹é‡å¤„ç†ç»“æœ ===")
for i, result in enumerate(batch_results):
    status = "âœ…" if result['match'] else "âŒ"
    print(f"{status} {result['original']} â†’ {result['extracted']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")

# 2. æ€§èƒ½æµ‹è¯•
import time

print(f"\n=== æ€§èƒ½æµ‹è¯• ===")
start_time = time.time()
for i in range(10):
    result = watermark.embed(model, tokenizer, f"Test {i}:", f"msg{i}")
    if result['success']:
        watermark.extract(result['watermarked_text'], model, tokenizer)
end_time = time.time()

print(f"10æ¬¡åµŒå…¥+æå–è€—æ—¶: {end_time - start_time:.2f}ç§’")
print(f"å¹³å‡æ¯æ¬¡: {(end_time - start_time)/10:.2f}ç§’")
```

**ğŸ¯ Day 3-5 æ€»ç»“:**
- âœ… **ä»£ç é›†æˆ**: æˆåŠŸå¤åˆ¶å¹¶ä¿®å¤CredIDæ ¸å¿ƒä»£ç 
- âœ… **æ¥å£å°è£…**: å®ç°ç»Ÿä¸€çš„embed/extractæ¥å£
- âœ… **å¤šæ¶ˆæ¯æ”¯æŒ**: æ”¯æŒå­—ç¬¦ä¸²ã€åˆ—è¡¨ã€å¤æ‚æ··åˆæ¶ˆæ¯
- âœ… **å€™é€‰ä¼˜åŒ–**: å®ç°å€™é€‰æ¶ˆæ¯é™åˆ¶æœç´¢æå‡æ•ˆç‡
- âœ… **é”™è¯¯å¤„ç†**: å®Œæ•´çš„å¼‚å¸¸å¤„ç†å’ŒçŠ¶æ€æŠ¥å‘Š
- âœ… **é…ç½®ç³»ç»Ÿ**: çµæ´»çš„YAMLé…ç½®ç®¡ç†
- âœ… **æµ‹è¯•éªŒè¯**: å¤šç§åœºæ™¯çš„åŠŸèƒ½æµ‹è¯•

**ä¸ºåç»­å¼€å‘æä¾›çš„å‚è€ƒæ¶æ„:**
1. **ç»Ÿä¸€æ¥å£æ¨¡å¼**: embed(model, tokenizer, prompt, message) â†’ extract(text, model, tokenizer)
2. **é…ç½®é©±åŠ¨è®¾è®¡**: é€šè¿‡YAMLæ–‡ä»¶ç®¡ç†ç®—æ³•å‚æ•°
3. **è¿”å›æ ¼å¼æ ‡å‡†**: ç»Ÿä¸€çš„æˆåŠŸ/å¤±è´¥çŠ¶æ€å’Œå…ƒæ•°æ®ç»“æ„
4. **å€™é€‰ä¼˜åŒ–æœºåˆ¶**: æ”¯æŒå€™é€‰åˆ—è¡¨çš„é«˜æ•ˆæœç´¢
5. **å¤šæ¨¡æ€æ¶ˆæ¯**: æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼çš„æ¶ˆæ¯ç¼–ç 

---

### ğŸ–¼ï¸ Day 6-8: Stable Signatureå›¾åƒæ°´å°é›†æˆ

#### Day 6: Stable Signatureä»£ç å¤åˆ¶
```bash
# å¤åˆ¶æ ¸å¿ƒæ–‡ä»¶
cp /path/to/stable_signature/utils*.py src/image_watermark/stable_sig/
cp /path/to/stable_signature/finetune_ldm_decoder.py src/image_watermark/stable_sig/
cp -r /path/to/stable_signature/src/* src/image_watermark/stable_sig/

# ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
wget https://dl.fbaipublicfiles.com/ssl_watermarking/dec_48b_whit.torchscript.pt -P models/
```

#### Day 7: Stable Signatureå°è£…ç±»å¼€å‘
å®ç° `src/image_watermark/stable_signature.py`

```python
# src/image_watermark/stable_signature.py
import torch
from PIL import Image
from typing import Dict, Any, Union
from diffusers import StableDiffusionPipeline

class StableSignatureWatermark:
    """Stable Signatureå›¾åƒæ°´å°ç®—æ³•å°è£…"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self._setup_models()
    
    def embed(self, prompt: str, message: str) -> Dict[str, Any]:
        """ç”Ÿæˆå¸¦æ°´å°å›¾åƒ"""
        # å®ç°å›¾åƒç”Ÿæˆå’Œæ°´å°åµŒå…¥
        pass
    
    def extract(self, watermarked_image: Union[Image.Image, str]) -> Dict[str, Any]:
        """æå–å›¾åƒæ°´å°"""
        # å®ç°æ°´å°æå–
        pass
```

#### Day 8: å›¾åƒæ°´å°æµ‹è¯•
**config/image_config.yaml**:
```yaml
method: "stable_signature"
message_length: 48
decoder_path: "models/dec_48b_whit.torchscript.pt"

diffusion:
  model_name: "stabilityai/stable-diffusion-2"
  num_inference_steps: 50
  guidance_scale: 7.5
  height: 512
  width: 512
```

---

### ğŸš€ Day 9-10: ç»Ÿä¸€å¼•æ“å’Œæ•´åˆ

#### Day 9: WatermarkEngineç»Ÿä¸€æ¥å£
å®ç° `src/watermark_engine.py`

```python
# src/watermark_engine.py
import os
from typing import Dict, Any
from .utils.config_loader import load_config
from .text_watermark.credid_watermark import CredIDWatermark
from .image_watermark.stable_signature import StableSignatureWatermark

class WatermarkEngine:
    """å¤šæ¨¡æ€æ°´å°ç»Ÿä¸€å¼•æ“"""
    
    def __init__(self, base_dir: str = "."):
        self.base_dir = base_dir
        self.text_watermark = None
        self.image_watermark = None
    
    def embed_text(self, model, tokenizer, prompt: str, message: str):
        """åµŒå…¥æ–‡æœ¬æ°´å°"""
        if not self.text_watermark:
            config = load_config(os.path.join(self.base_dir, "config/text_config.yaml"))
            self.text_watermark = CredIDWatermark(config)
        return self.text_watermark.embed(model, tokenizer, prompt, message)
    
    def extract_text(self, watermarked_text: str):
        """æå–æ–‡æœ¬æ°´å°"""
        if not self.text_watermark:
            config = load_config(os.path.join(self.base_dir, "config/text_config.yaml"))
            self.text_watermark = CredIDWatermark(config)
        return self.text_watermark.extract(watermarked_text)
    
    def embed_image(self, prompt: str, message: str):
        """åµŒå…¥å›¾åƒæ°´å°"""
        if not self.image_watermark:
            config = load_config(os.path.join(self.base_dir, "config/image_config.yaml"))
            self.image_watermark = StableSignatureWatermark(config)
        return self.image_watermark.embed(prompt, message)
    
    def extract_image(self, watermarked_image):
        """æå–å›¾åƒæ°´å°"""
        if not self.image_watermark:
            config = load_config(os.path.join(self.base_dir, "config/image_config.yaml"))
            self.image_watermark = StableSignatureWatermark(config)
        return self.image_watermark.extract(watermarked_image)
```

#### Day 10: æœ€ç»ˆæµ‹è¯•å’Œæ–‡æ¡£
åˆ›å»º `examples/unified_demo.py`

```python
# examples/unified_demo.py
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.watermark_engine import WatermarkEngine
from transformers import AutoModelForCausalLM, AutoTokenizer

def main():
    print("ğŸŒŠ å¤šæ¨¡æ€æ°´å°å·¥å…·æ¼”ç¤º")
    
    # åˆå§‹åŒ–å¼•æ“
    engine = WatermarkEngine()
    
    # æ–‡æœ¬æ°´å°æ¼”ç¤º
    print("\nğŸ“ æ–‡æœ¬æ°´å°æµ‹è¯•...")
    model = AutoModelForCausalLM.from_pretrained("huggyllama/llama-7b")
    tokenizer = AutoTokenizer.from_pretrained("huggyllama/llama-7b")
    
    result = engine.embed_text(model, tokenizer, "The weather today is", "hello")
    print(f"æ°´å°æ–‡æœ¬: {result}")
    
    extracted = engine.extract_text(result['watermarked_text'])
    print(f"æå–ç»“æœ: {extracted}")
    
    # å›¾åƒæ°´å°æ¼”ç¤º
    print("\nğŸ–¼ï¸ å›¾åƒæ°´å°æµ‹è¯•...")
    image_result = engine.embed_image("a beautiful cat", "secret")
    print(f"å›¾åƒç”Ÿæˆ: {image_result}")
    
    extracted = engine.extract_image(image_result['watermarked_image'])
    print(f"æå–ç»“æœ: {extracted}")

if __name__ == "__main__":
    main()
```
Ran tool